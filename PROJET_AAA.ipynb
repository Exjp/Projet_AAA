{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Machine Learning in Network Intrusion Detection Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'article utilise comme dataset NSL-KDD et UNSW-NB15, qui représentent des flux de données réseau bruts. Ces flux sont un mélange de différents types de trafic, mais également un mélange de bonnes et malicieuses données. Le but étant d'altérer ces paquets réseau pour pouvoir déjouer les systèmes de detection d'intrusion, tout en gardant les flux de données fonctionnels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"KDDTest+.csv\")\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
    "#df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Genetic algorithm* (GA) est un algorithme qui se base sur la génétique et la sélection naturelle. Le GA utilise une forme de données appelées chromosome : ces données (vecteurs) sont formées et les valeurs de différentes features sont destinées à évoluer comme pour un gêne d'être vivant.\n",
    "Certaines *features* sont inchangeable du à leur importance dans le bon fonctionnement des paquets.\n",
    "\n",
    "- En premier lieu, on va créer une population de 100 chromosomes, dont leurs données seront générer aléatoirement, séparées en 2 parties, les données que l'ont peu modifier et les autres. (pour les opérations qui suivent, on va utiliser la partie que l'on peut modifier)\n",
    "- on va ensuite utiliser l'opération de *cross over*. On choisi aléatoirement 2 chromosomes que l'on va séparer l'un de 25% de ses features et donc, de l'autre des 75% autres, que l'on va échanger pour créer 2 nouveau chromosomes. Ces nouveaux chromosomes seront ensuite insérés dans la population.\n",
    "- Enfin, on va utiliser l'opération de *mutation* sur la partie que l'on peut modifier, qui consiste à choisir de manière aléatoire un chromosome et de même pour une de ses features. Toute cette opération est exécutée \n",
    "- On regroupe les 2 parties des chromosomes.\n",
    "- Finalement, on va utiliser une *fitness function* pour évaluer chaque chromosome, s'ils sont bénins (seuil à 99,99%) ou non. On retiendra le meilleur chromosome toutes les 5 générations.\n",
    "- On répète tout ce processus 100 fois\n",
    "On obtient alors un nouveau dataset ne contenant que des vecteurs \"malsains\" qui seront testés dans la partie expérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geneticalgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Creation of initial population\n",
    "listProtocols = [\"tcp\",\"udp\",\"icmp\"]\n",
    "listData = [\"private\",\"ftp_data\",\"eco_i\",\"telnet\",\"http\",\"smtp\",\"imap4\",\"systat\",\"pop3\",\"domain_u\",\"whois\",\"netbios_dgm\"]\n",
    "listSF = [\"SF\",\"S0\",\"REJ\",\"RSTR\"]\n",
    "#cell 3 = 62825648 - 0\n",
    "# 24 à 30 et 33 à 40 entre 0 et 1\n",
    "# 4 à 23 , 31- 32,  entre 0 et 20\n",
    "#derniere = 0 - 21\n",
    "\n",
    "# creation of artifical dataset\n",
    "def initialPopulation(df, nbChrom):\n",
    "    initPopulation = []\n",
    "    for i in range (nbChrom):\n",
    "        chrom = []\n",
    "        for j in range (len(df.columns)):\n",
    "            if(j == 0) :\n",
    "                chrom.insert(j,random.randint(0, 2))\n",
    "            if(j == 1) :\n",
    "                chrom.insert(j,random.randint(0, 11))\n",
    "            if(j == 2) :\n",
    "                chrom.insert(j,randint(0, 62825648))\n",
    "            if((j >= 24 and j <= 30) or (j >= 33 and j <= 40)) :\n",
    "                chrom.insert(j,random.uniform(0, 1))\n",
    "            if((j >= 4 and j <= 23) or j == 32 or j ==33) :\n",
    "                chrom.insert(j,random.randint(0, 62825648))\n",
    "            if(j == 41) :\n",
    "                chrom.insert(j,random.randint(0,21))\n",
    "        initPopulation.append(chrom)\n",
    "    return initPopulation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimDF = len(df.columns)\n",
    "dimDF2 = len(df2.columns)\n",
    "\n",
    "#We have to reduce to a minimum data size otherwise our computers can not run the fit after...\n",
    "dfCopy = df.copy()\n",
    "dfCopy = dfCopy.head(1000)\n",
    "\n",
    "# preparation of the data : transforming string input to int \n",
    "LE = LabelEncoder()\n",
    "dfCopy['protocol_type'] = LE.fit_transform(dfCopy['protocol_type'])\n",
    "dfCopy['service'] = LE.fit_transform(dfCopy['service'])\n",
    "dfCopy['flag'] = LE.fit_transform(dfCopy['flag'])\n",
    "dfCopy['class'] = LE.fit_transform(dfCopy['class'])\n",
    "dfCopy.head()\n",
    "\n",
    "y = dfCopy['class']\n",
    "dfCopy.drop(['class'], axis='columns', inplace=True)\n",
    "X = dfCopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = initialPopulation(dfCopy,100)\n",
    "#fitnessScore(population,)\n",
    "#population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TO ADAPT\\ndef selection(pop_after_fit,n_parents):\\n    population_nextgen = []\\n    for i in range(n_parents):\\n        population_nextgen.append(pop_after_fit[i])\\n    return population_nextgen\\n\\ndef crossover(pop_after_sel):\\n    population_nextgen=pop_after_sel\\n    for i in range(len(pop_after_sel)):\\n        child=pop_after_sel[i]\\n        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\\n        population_nextgen.append(child)\\n    return population_nextgen\\n\\ndef mutation(pop_after_cross,mutation_rate):\\n    population_nextgen = []\\n    for i in range(0,len(pop_after_cross)):\\n        chromosome = pop_after_cross[i]\\n        for j in range(len(chromosome)):\\n            if random.random() < mutation_rate:\\n                chromosome[j]= not chromosome[j]\\n        population_nextgen.append(chromosome)\\n    #print(population_nextgen)\\n    return population_nextgen\\n\\ndef generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\\n                                   X_test, y_train, y_test):\\n    best_chromo= []\\n    best_score= []\\n    population_nextgen=initilization_of_population(size,n_feat)\\n    for i in range(n_gen):\\n        scores, pop_after_fit = fitness_score(population_nextgen)\\n        print(scores[:2])\\n        pop_after_sel = selection(pop_after_fit,n_parents)\\n        pop_after_cross = crossover(pop_after_sel)\\n        population_nextgen = mutation(pop_after_cross,mutation_rate)\\n        best_chromo.append(pop_after_fit[0])\\n        best_score.append(scores[0])\\n    return best_chromo,best_score\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters given in the section 4.3 of the article\n",
    "svm = svm.SVC(C = 220, gamma = 0.01, probability = True, tol = 0.001)\n",
    "#  decision trees\n",
    "dectr = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 4, \\\n",
    "                                    min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "#naive bayes\n",
    "nb = GaussianNB()\n",
    "#k nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, algorithm ='auto')\n",
    "\n",
    "estimators = [ ('svm', svm), ('dt', dectr), ('nb', nb), ('knn', knn)]\n",
    "model = ensemble.VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "fitModel = model.fit(X,y)\n",
    "\n",
    "\n",
    "# https://datascienceplus.com/genetic-algorithm-in-machine-learning-using-python/\n",
    "\n",
    "def fitnessScore(population,yTest):\n",
    "    scores = []\n",
    "    for chromosome in population :\n",
    "        prediction = fitModel.predict(chromosome)\n",
    "        # We want the bad pred to be valued better\n",
    "        # 0 correspond to \"anomaly\" and 1 to \"normal\"\n",
    "        if(prediction == 0):\n",
    "            score = 1\n",
    "        else :\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    scores, population = np.array(scores), np.array(population) \n",
    "    inds = np.argsort(scores)\n",
    "    return list(scores[inds][::-1]), list(population[inds,:][::-1])\n",
    "\n",
    "''' # TO ADAPT\n",
    "def selection(pop_after_fit,n_parents):\n",
    "    population_nextgen = []\n",
    "    for i in range(n_parents):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "\n",
    "def crossover(pop_after_sel):\n",
    "    population_nextgen=pop_after_sel\n",
    "    for i in range(len(pop_after_sel)):\n",
    "        child=pop_after_sel[i]\n",
    "        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n",
    "        population_nextgen.append(child)\n",
    "    return population_nextgen\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate):\n",
    "    population_nextgen = []\n",
    "    for i in range(0,len(pop_after_cross)):\n",
    "        chromosome = pop_after_cross[i]\n",
    "        for j in range(len(chromosome)):\n",
    "            if random.random() < mutation_rate:\n",
    "                chromosome[j]= not chromosome[j]\n",
    "        population_nextgen.append(chromosome)\n",
    "    #print(population_nextgen)\n",
    "    return population_nextgen\n",
    "\n",
    "def generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n",
    "                                   X_test, y_train, y_test):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(size,n_feat)\n",
    "    for i in range(n_gen):\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "        print(scores[:2])\n",
    "        pop_after_sel = selection(pop_after_fit,n_parents)\n",
    "        pop_after_cross = crossover(pop_after_sel)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "    return best_chromo,best_score\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but est de maximiser le nombre de vecteurs malsain classés en sain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarn Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du *Particle Swarm Optimization* est que pour chaque particule d'une nuée, on va essayer de trouver la meilleure position retournée par la *fitness function*. \n",
    "\n",
    "- D'abord, on doit créer la nuée de 200 particules. Les particules sont formées à partir d'une rangée du dataset et d'une partie généré aléatoirement, toujours avec les mêmes *features* que le dataset. Comme que pour le GA, on devrait prendre seulement la partie mutable des particules à modifier. Ensuites, pour chaque particules est attribué une vélocité de 0.7.\n",
    "- Pour chaque itération, on va évaluer chaque particule à partir de la *fitness function*. On met à jour la meilleur valeur de la fonction fitness possible de chaque particules (de 0.5 au départ) et de la meilleur valeur de la fonction fitness global (de 0.4 au départ)\n",
    "- On calcule en suite la distance entre la position de chaque particule et on récupère la meilleure valeur grâce à la fonction de fitness. On met à jour la vélocité et la position de chaque particule avec les valeurs précédentes.\n",
    "- On répète tout ce processus 100 fois au maximum ou s'il y a une amélioration de moins de 0.001%.\n",
    "La nué de particules devient alors un nouveau dataset de vecteurs *malsains*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarms.single import GlobalBestPSO\n",
    "from pyswarms.utils.search.grid_search  import SearchBase\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def prepare_UNSW_NB15(data):\n",
    "    LE = LabelEncoder()\n",
    "    copy = data[data['label']==1]\n",
    "    copy = copy.drop(['label'], axis=1)\n",
    "    copy['proto'] = LE.fit_transform(copy['proto'])\n",
    "    copy['service'] = LE.fit_transform(copy['service'])\n",
    "    copy['state'] = LE.fit_transform(copy['state'])\n",
    "    copy['attack_cat'] = LE.fit_transform(copy['attack_cat'])\n",
    "    return copy\n",
    "\n",
    "def voting_classifier(X_train, Y_train) :\n",
    "  sv = svm.SVC(gamma = 0.01, C = 220.0, tol = 0.01, probability = True)\n",
    "  dt = DecisionTreeClassifier(\n",
    "      criterion = \"entropy\", min_samples_split = 4, min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "  nb = GaussianNB()\n",
    "  kn = KNeighborsClassifier( n_neighbors = 3, algorithm = \"auto\")\n",
    "\n",
    "  vC = VotingClassifier (estimators=\n",
    "                         [('svm', sv), ('dt', dt), ('nb', nb), ('knn',kn)], voting='soft')\n",
    "  vC = vC.fit(X_train,Y_train)\n",
    "  return vC\n",
    "\n",
    "def Pso (dim, data) :\n",
    "  option = {'c1': 0.5, 'c2': 0.4, 'w' : 0.7}\n",
    "  opt = GlobalBestPSO (n_particles=200, dimensions=dim, options=option)\n",
    "  return opt.optimize(prepare_UNSW_NB15, 100, None, True, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#df2prepare = Pso (len(copy.columns), df2)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df2prepare, test_size=0.2, random_state=300)\n",
    "\n",
    "#voting_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Generative Adversial Network* est une technique de deep learning qui va confronter deux réseaux de neuronnes. Ces deux réseaux de neuronnes correspondent l'un à la génération des vecteur *malicieux* et donc à ceux qu'elles permettent de tromper, l'autre est le discriminateur, qui lui va essayer de deviner si l'entrée est saine ou non.\n",
    "Comme les méthodes précédentes, on va modifier d'abord la partie modifiable, c'est à dire non fixée des vecteurs. \n",
    "- Pour le générateur, on va sélectionner aléatoirement un vecteur \"malsain\". On va ajouter du bruit dans certaines des features choisies au hasard, toujours dans la partie modifiable du vecteur malicieux. Puis ce vecteur va rentrer dans le réseau neuronal du générateur avec comme sortie, un vecteur de la taille de la partie mutable du vecteur d'entrée. On recombine le vecteur de sortie avec la partie non changeable du vecteur de l'entrée, et on le donne en entrée du discriminateur. Les labels sortant de ce dernier permettrons d'améliorer le réseau de neurone du générateur.\n",
    "- Pour le discriminateur, on va faire la même chose avec mais un vecteur bénin.\n",
    "- Les deux réseaux de neurones utiliserons des sets de données d'entraînements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les expériences sont enfin réalisée sur les 11 modèles de classification en utilisant les sets produits précédemment. \n",
    "L'article ne mentionne pas les hyperparamètres à appliquer pour les 11 classifieurs. \n",
    "Les 11 classifieurs sont Support Vector Machine (SVM), Decision Trees (DT), Naive Bayes (NB), k Nearest Neighbors (KNN), Random Forest (RF), Multi-layer Perceptron (MLP), Gradient Boosting (GB), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Bagging (BAG)\n",
    "Nous laisserons principalement les paramètres par défaut pour réaliser les expériences.\n",
    "Les résultats attendus pour ces expériences sont un taux élevé de mauvaise classification (dans le cas où nous arriverions à générer les adversairal datasets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import mean,std\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_UNSW_NB15(df):\n",
    "    dfcopy = df.copy()\n",
    "    dfcopy = df.sample(n = 10000)\n",
    "    LE = LabelEncoder()\n",
    "    #label = dfcopy.loc[:,('label')]\n",
    "    #dfcopy = dfcopy.drop(['label'], axis=1)\n",
    "    dfcopy['proto'] = LE.fit_transform(dfcopy['proto'])\n",
    "    dfcopy['service'] = LE.fit_transform(dfcopy['service'])\n",
    "    dfcopy['state'] = LE.fit_transform(dfcopy['state'])\n",
    "    dfcopy['attack_cat'] = LE.fit_transform(dfcopy['attack_cat'])\n",
    "    dfcopy['label'] = LE.fit_transform(dfcopy['label'])\n",
    "    \n",
    "    y = dfcopy['label']\n",
    "    dfcopy.drop(['label'], axis='columns', inplace=True)\n",
    "    return dfcopy, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareKDD(df):\n",
    "    trainCopy = df.copy()\n",
    "    # limit cause we can't run it on 22000 entries.\n",
    "    trainCopy = df.sample(n = 10000)\n",
    "\n",
    "    # preparation of the data : transforming string input to int \n",
    "    LE = LabelEncoder()\n",
    "    trainCopy['protocol_type'] = LE.fit_transform(trainCopy['protocol_type'])\n",
    "    trainCopy['service'] = LE.fit_transform(trainCopy['service'])\n",
    "    trainCopy['flag'] = LE.fit_transform(trainCopy['flag'])\n",
    "    trainCopy['class'] = LE.fit_transform(trainCopy['class'])\n",
    "    \n",
    "    y = trainCopy['class']\n",
    "    trainCopy.drop(['class'], axis='columns', inplace=True)\n",
    "    \n",
    "    return trainCopy, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(clf, X_train, Y_train, X_test, Y_test):\n",
    "    model = clf();\n",
    "    model.fit(X_train, Y_train);\n",
    "    msKDD = missClassifiedAsNormalKDD(Y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(Y_test, X_test, model)\n",
    "    return model.score(X_test, Y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparaison (clf, X_train, Y_train, X_test, Y_test) :\n",
    "    scores = []\n",
    "    scoresMsKDD = []\n",
    "    scoresMsUNSW = []\n",
    "    for i in clf :\n",
    "        score, msKDD, msUNSW = classifier(i, X_train, Y_train, X_test, Y_test)\n",
    "        scores.append(score)\n",
    "        scores.append(msKDD)\n",
    "        scores.append(msUNSW)\n",
    "    return scores, scoresMsKDD, scoresMsUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingexp(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #df2copy = prepareDataset(dataset)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df2copy, label, test_size=0.5, random_state=500)\n",
    "    \n",
    "\n",
    "    model = BaggingClassifier(base_estimator=None, n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "#     n_scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "    #missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMc(X_train, y_train, X_test, y_test):\n",
    "    model = SVC()\n",
    "    model.fit(X_train,y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA(X_train, y_train, X_test, y_test):\n",
    "    model = QuadraticDiscriminantAnalysis()\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA(X_train, y_train, X_test, y_test):\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(X_train, y_train, X_test, y_test):\n",
    "    model = LogisticRegression(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradBoost(X_train, y_train, X_test, y_test):\n",
    "    model = GradientBoostingClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(X_train, y_train, X_test, y_test):\n",
    "    model = MLPClassifier(random_state=0, max_iter=10)\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X_train, y_train, X_test, y_test):\n",
    "    model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    msKDD = missClassifiedAsNormalKDD(y_test, X_test, model)\n",
    "    msUNSW = missClassifiedAsNormalUNSW(y_test, X_test, model)\n",
    "    return model.score(X_test, y_test), msKDD, msUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClassifieurBarScore(score):\n",
    "    classifieur = []\n",
    "    for i in range (len(score)):\n",
    "#         classifieur.insert(i,i)\n",
    "        score[i] = (1 - score[i])*100\n",
    "    plotDF = pd.DataFrame({'Classifieur':clfName, 'Score':score})\n",
    "    plotDF.plot.bar(x=\"Classifieur\", y='Score', rot=0,figsize=(16,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the global score, and the missclassied score for both datasets \n",
    "def all_classifier_score(X_train, X_test, y_train, y_test) :\n",
    "    clf = [DecisionTreeClassifier, GaussianNB, KNeighborsClassifier]\n",
    "    clfName = [\"DTC\", \"GaussianNB\", \"KNC\"]\n",
    "    scores, scoresMsKDD, scoresMsUNSW = comparaison(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    score, msKDD, msUNSW = SVMc(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"SVM\")\n",
    "    \n",
    "    score, msKDD, msUNSW = baggingexp(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"Bagging\")\n",
    "    \n",
    "    score, msKDD, msUNSW = QDA(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"QDA\")\n",
    "    \n",
    "    score, msKDD, msUNSW = logisticRegression(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"logisticRegression\")\n",
    "    \n",
    "    score, msKDD, msUNSW = LDA(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"LDA\")\n",
    "    \n",
    "    score, msKDD, msUNSW = gradBoost(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"gradBoost\")\n",
    "    \n",
    "    score, msKDD, msUNSW = randomForest(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"randomForest\")\n",
    "    \n",
    "    score, msKDD, msUNSW = MLP(X_train,y_train, X_test, y_test)\n",
    "    scores.append(score)\n",
    "    scoresMsKDD.append(msKDD)\n",
    "    scoresMsUNSW.append(msUNSW)\n",
    "    clfName.append(\"MLP\")\n",
    "    return scores, clfName, scoresMsKDD, scoresMsUNSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missClassifiedAsNormalUNSW(y_test, X_test, model):\n",
    "    #msKDD = np.where(y_test == 'anomaly'and model.predict(X_test) == 'normal')\n",
    "    predicted = model.predict(X_test)\n",
    "    predicted = np.asarray(predicted)\n",
    "    y_test = np.asarray(y_test)\n",
    "    missClassified = 0\n",
    "    for i in range (len(y_test)):\n",
    "        # 1 = attack, 0 = normal \n",
    "        if(y_test[i] == 1 and predicted[i] == 0):\n",
    "            missClassified += 1\n",
    "    if(missClassified == 0) :\n",
    "        return missClassified;\n",
    "    else:\n",
    "        missClassified = len(y_test) / missClassified\n",
    "        return missClassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missClassifiedAsNormalKDD(y_test, X_test, model):\n",
    "    #msKDD = np.where(y_test == 'anomaly'and model.predict(X_test) == 'normal')\n",
    "    predicted = model.predict(X_test)\n",
    "    predicted = np.asarray(predicted)\n",
    "    y_test = np.asarray(y_test)\n",
    "    missClassified = 0\n",
    "    for i in range (len(y_test)):\n",
    "        # 0 = normal \n",
    "        if(y_test[i] == 0 and predicted[i] == 1):\n",
    "            missClassified += 1\n",
    "    if(missClassified == 0) :\n",
    "        return missClassified;\n",
    "    else:\n",
    "        missClassified = len(y_test) / missClassified\n",
    "        return missClassified   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données adversial KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange, random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def modify_sample(df, col, val) :\n",
    "    try :\n",
    "        mod = df.sample(n=500)\n",
    "    except :\n",
    "        mod = df.sample(frac=0.9)\n",
    "    if (val!=None):\n",
    "        mod[col] = val\n",
    "    else :\n",
    "        mod[col] = mod[col].apply(lambda x :x+randrange(500))\n",
    "    mod['class'] = 'anomaly'\n",
    "    return mod\n",
    "\n",
    "def related_sample(df, col1, col2) :\n",
    "    mod = df.sample(n=randrange(500))\n",
    "    mod[col1] = mod[col1].apply(lambda x : random())\n",
    "    mod[col2] = mod[col2].apply(lambda x: 1 - mod[col1])\n",
    "    mod['class'] = 'anomaly'\n",
    "    return mod\n",
    "    \n",
    "def binary_sample(df, col) :\n",
    "    on = modify_sample(df[df[col]==0], col, 1)\n",
    "    off = modify_sample(df[df[col]==1], col, 0)\n",
    "    return on, off\n",
    "\n",
    "def adversial_kdd(df) :\n",
    "    dfCopy = df.copy()\n",
    "    dfCopy = dfCopy[dfCopy['class']=='normal']\n",
    "    frames = [dfCopy]\n",
    "    #Possible modification given in the paper\n",
    "    src_bytes_null = modify_sample(dfCopy, 'src_bytes', 0)\n",
    "    frames.append(src_bytes_null)\n",
    "    src_bytes_increase = modify_sample(dfCopy, 'src_bytes', None)\n",
    "    frames.append(src_bytes_increase)\n",
    "    dst_bytes_null = modify_sample(dfCopy, 'dst_bytes', 0)\n",
    "    frames.append(dst_bytes_null)\n",
    "    dst_bytes_increase = modify_sample(dfCopy, 'dst_bytes', None)\n",
    "    frames.append(dst_bytes_increase)\n",
    "    svr_rate = related_sample(dfCopy, 'same_srv_rate', 'diff_srv_rate')\n",
    "    frames.append(svr_rate)\n",
    "    # Binary modification\n",
    "    logged_in_on, logged_in_off = binary_sample(dfCopy, 'logged_in')\n",
    "    frames.append(logged_in_on)\n",
    "    frames.append(logged_in_off)\n",
    "    root_shell_on, root_shell_off = binary_sample(dfCopy, 'root_shell')\n",
    "    frames.append(root_shell_on)\n",
    "    frames.append(root_shell_off)\n",
    "    #\n",
    "    adversial = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    adversial = shuffle(adversial)\n",
    "    adversial.reset_index(inplace=True, drop=True)\n",
    "    return adversial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfad = adversial_kdd(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appels de tous les classifieurs et récupération des résultats (dataset UNSW_NB15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vasaw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8924242424242425, 23.404255319148938, 15.42056074766355, 0.7203030303030303, 4.029304029304029, 31.73076923076923, 0.7909090909090909, 5.679862306368331, 30.275229357798164, 0.7384848484848485, 0.9303030303030303, 0.7427272727272727, 0.7381818181818182, 0.7748484848484849, 0.9390909090909091, 0.74, 0.7627272727272727]\n",
      "[3.82830626450116, 24.264705882352942, 8.048780487804878, 3.82830626450116, 4.602510460251046, 17.010309278350515, 3.8461538461538463, 4.954954954954955]\n",
      "[3300.0, 35.1063829787234, 7.517084282460137, 1650.0, 126.92307692307692, 471.42857142857144, 0, 28.205128205128204]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vasaw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X, y = prepareKDD(dfad)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "#print(y_test)\n",
    "scores, clfName, scoresMsKDD, scoresMsUNSW = all_classifier_score(X_train, X_test, y_train, y_test)\n",
    "#score\n",
    "#plotClassifieurBarScore(score)\n",
    "print(scores)\n",
    "print(scoresMsKDD)\n",
    "print(scoresMsUNSW)\n",
    "\n",
    "#plotClassifieurBarScore(scoresMsKDD)\n",
    "#plotClassifieurBarScore(scoresMsUNSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données adversial UNSW-NB15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_sample(df, col, val) :\n",
    "    try :\n",
    "        mod = df.sample(n=500)\n",
    "    except :\n",
    "        mod = df.sample(frac=0.9)\n",
    "    if (val!=None):\n",
    "        mod[col] = val\n",
    "    else :\n",
    "        mod[col] = mod[col].apply(lambda x :x+randrange(500))\n",
    "    mod['label'] = 1\n",
    "    return mod\n",
    "\n",
    "def twoValSample(df, col) :\n",
    "    sp1 = modify_sample(df[df[col] == 0], col, 255)\n",
    "    sp2 = modify_sample(df[df[col] == 255], col, 0)\n",
    "    return sp1,sp2\n",
    "\n",
    "def adversial_UNSWNB15(df):\n",
    "    dfCopy = df.copy()\n",
    "    #normal state\n",
    "    dfCopy = dfCopy[dfCopy['label'] == 0]\n",
    "    frames = [dfCopy]\n",
    "    \n",
    "    #perturbations\n",
    "    stcpb_increase = modify_sample(dfCopy, 'stcpb', 50000)\n",
    "    frames.append(stcpb_increase)\n",
    "    dtcpb_increase = modify_sample(dfCopy, 'dtcpb', 50000)\n",
    "    frames.append(dtcpb_increase)\n",
    "    swin_reverse1, swin_reverse2 = twoValSample(dfCopy, 'swin')\n",
    "    frames.append(swin_reverse1)\n",
    "    frames.append(swin_reverse2)\n",
    "    dwin_reverse1,dwin_reverse2 = twoValSample(dfCopy, 'dwin')\n",
    "    frames.append(dwin_reverse1)\n",
    "    frames.append(dwin_reverse2)\n",
    "    ackdat_increase = modify_sample(dfCopy, 'ackdat', 0.05)\n",
    "    frames.append(ackdat_increase)\n",
    "    sbytes_increase = modify_sample(dfCopy, 'sbytes', 1000)\n",
    "    frames.append(sbytes_increase)\n",
    "    dbytes_increase = modify_sample(dfCopy, 'dbytes', 1000)\n",
    "    frames.append(dbytes_increase)\n",
    "    spkts_increase = modify_sample(dfCopy, 'spkts', 1000)\n",
    "    frames.append(spkts_increase)\n",
    "    dpkts_increase = modify_sample(dfCopy, 'dpkts', 1000)\n",
    "    frames.append(dpkts_increase)\n",
    "    ct_dst_src_ltm_increase = modify_sample(dfCopy, 'ct_dst_src_ltm', 6)\n",
    "    frames.append(ct_dst_src_ltm_increase)\n",
    "    \n",
    "    #retrieving final adversarial dataset \n",
    "    adversial = pd.concat(frames, ignore_index=True, sort=False)\n",
    "    adversial = shuffle(adversial)\n",
    "    adversial.reset_index(inplace=True, drop=True)\n",
    "    return adversial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfad2 = adversial_UNSWNB15(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appels de tous les classifieurs et récupération des résultats (dataset UNSW_NB15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vasaw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\discriminant_analysis.py:715: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "c:\\users\\vasaw\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAAE9CAYAAABjmL2OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlV0lEQVR4nO3de5glZX0u7OcngwwKEoEJIiCDioiGUxyIJwxyigYT9RODbKNg9CMxEcVt3CEaDZrPBKPGRDwFlGB2EImiwQCXoggbVGIYYOQgCkZBcSMnRQElcni/P6oaFm33TM90V/d0931fV19dq1Yd3lWrTk+9b9Wq1loAAABgKA+Z6wIAAACwsAmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCglszmzLbccsu2fPny2ZwlAAAAs+Tiiy++pbW2bHz/WQ2ey5cvz8qVK2dzlgAAAMySqrpuov6a2gIAADAowRMAAIBBCZ4AAAAMalbv8ZzI3Xffneuvvz533XXXXBdl1i1dujTbbrttNtxww7kuCgAAwGDmPHhef/312XTTTbN8+fJU1VwXZ9a01nLrrbfm+uuvzw477DDXxQEAABjMnDe1veuuu7LFFlssqtCZJFWVLbbYYlHW9AIAAIvLnAfPJIsudI5ZrJ8bAABYXNaL4Lk+eMc73pEnP/nJ2XXXXbP77rvna1/72lwXCQAAYEGY83s8x1t+9JkzOr1rjz1ojcNceOGFOeOMM3LJJZdko402yi233JJf/OIX6zzPe+65J0uWrHeLFgAAYE6o8Uxyww03ZMstt8xGG22UJNlyyy3z6Ec/OhdddFGe/vSnZ7fddstee+2V22+/PXfddVde8YpXZJdddskee+yRc889N0ly0kkn5Xd/93ez7777Zr/99sudd96ZP/iDP8hee+2VPfbYI6effvpcfkQAAIA5o1ouyYEHHpi3v/3tecITnpD9998/hxxySJ72tKflkEMOyamnnpo999wzP/3pT7PxxhvnH/7hH1JVufzyy/PNb34zBx54YK6++uokySWXXJLLLrssm2++ed70pjdl3333zYknnpjbbrste+21V/bff/88/OEPn+NPCwAAMLsEzySbbLJJLr744lxwwQU599xzc8ghh+TNb35ztt566+y5555Jkkc84hFJki9/+cs58sgjkyRPfOITs/32298fPA844IBsvvnmSZKzzz47n/3sZ/Pud787Sff03u9973vZeeedZ/vjAQAAc2CmbyOcaVO5LXGmCJ69DTbYIPvss0/22Wef7LLLLvnABz6w1tMYrc1sreW0007LTjvtNJPFBAAAmHfc45nkW9/6Vq655pr7X69atSo777xzbrjhhlx00UVJkttvvz333HNP9t5775x88slJkquvvjrf+973JgyXv/Vbv5XjjjsurbUkyaWXXjoLnwQAAGD9o8YzyR133JEjjzwyt912W5YsWZLHP/7xOf744/OKV7wiRx55ZH7+859n4403zhe/+MX88R//cV796ldnl112yZIlS3LSSSfd/1CiUW95y1ty1FFHZdddd819992XHXbYIWecccYcfDoAAIC5VWM1crNhxYoVbeXKlQ/qd9VVVy3q+x4X++cHAICFajHe41lVF7fWVozvr6ktAAAAgxI8AQAAGJTgCQAAwKDWi+A5m/eZrk8W6+cGAAAWlzkPnkuXLs2tt9666EJYay233nprli5dOtdFAQAAGNSc/5zKtttum+uvvz4333zzXBdl1i1dujTbbrvtXBcDAABgUHMePDfccMPssMMOc10MAAAABjLnTW0BAABY2ARPAAAABiV4AgAAMCjBEwAAgEEJngAAAAxK8AQAAGBQgicAAACDEjwBAAAYlOAJAADAoARPAAAABrXG4FlV21XVuVX1jaq6sqpe1/c/pqp+UFWr+r/fHr64AAAAzDdLpjDMPUne0Fq7pKo2TXJxVX2hf++9rbV3D1c8AAAA5rs1Bs/W2g1Jbui7b6+qq5JsM3TBAAAAWBjW6h7PqlqeZI8kX+t7vaaqLquqE6vqkTNdOAAAAOa/KQfPqtokyWlJjmqt/TTJh5I8Lsnu6WpE3zPJeEdU1cqqWnnzzTdPv8QAAADMK1MKnlW1YbrQeXJr7dNJ0lq7sbV2b2vtviQnJNlronFba8e31la01lYsW7ZspsoNAADAPDGVp9pWko8muaq19ncj/bceGeyFSa6Y+eIBAAAw303lqbbPSPKyJJdX1aq+35uSHFpVuydpSa5N8ocDlA8AAIB5bipPtf1ykprgrbNmvjgAAAAsNGv1VFsAAABYW4InAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADCoNQbPqtquqs6tqm9U1ZVV9bq+/+ZV9YWquqb//8jhiwsAAMB8M5Uaz3uSvKG19qQkT03yJ1X1pCRHJzmntbZjknP61wAAAPAgawyerbUbWmuX9N23J7kqyTZJnp/kY/1gH0vygoHKCAAAwDy2Vvd4VtXyJHsk+VqSrVprN/Rv/TDJVjNbNAAAABaCKQfPqtokyWlJjmqt/XT0vdZaS9ImGe+IqlpZVStvvvnmaRUWAACA+WdKwbOqNkwXOk9urX26731jVW3dv791kpsmGre1dnxrbUVrbcWyZctmoswAAADMI1N5qm0l+WiSq1prfzfy1meTHNZ3H5bk9JkvHgAAAPPdkikM84wkL0tyeVWt6vu9KcmxSf61ql6Z5LokvzdICQEAAJjX1hg8W2tfTlKTvL3fzBYHAACAhWatnmoLAAAAa0vwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCg1hg8q+rEqrqpqq4Y6XdMVf2gqlb1f789bDEBAACYr6ZS43lSkudM0P+9rbXd+7+zZrZYAAAALBRrDJ6ttfOT/GgWygIAAMACNJ17PF9TVZf1TXEfOWMlAgAAYEFZ1+D5oSSPS7J7khuSvGeyAavqiKpaWVUrb7755nWcHQAAAPPVOgXP1tqNrbV7W2v3JTkhyV6rGfb41tqK1tqKZcuWrWs5AQAAmKfWKXhW1dYjL1+Y5IrJhgUAAGBxW7KmAarqlCT7JNmyqq5P8pdJ9qmq3ZO0JNcm+cPhiggAAMB8tsbg2Vo7dILeHx2gLAAAACxA03mqLQAAAKyR4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCglsx1AZhby48+c66LsEbXHnvQXBcBAACYBjWeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAINaY/CsqhOr6qaqumKk3+ZV9YWquqb//8hhiwkAAMB8NZUaz5OSPGdcv6OTnNNa2zHJOf1rAAAA+CVrDJ6ttfOT/Ghc7+cn+Vjf/bEkL5jZYgEAALBQLFnH8bZqrd3Qd/8wyVYzVB5gkVp+9JlzXYTVuvbYg+a6CDAvrO/bcmJ7hqla37dn2/L8sq7B836ttVZVbbL3q+qIJEckyWMe85jpzg7WS3bMAAAwuXV9qu2NVbV1kvT/b5pswNba8a21Fa21FcuWLVvH2QEAADBfrWvw/GySw/ruw5KcPjPFAQAAYKGZys+pnJLkwiQ7VdX1VfXKJMcmOaCqrkmyf/8aAAAAfska7/FsrR06yVv7zXBZAAAAWIDWtaktAAAATIngCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAg1oy1wUAAOABy48+c66LsEbXHnvQXBcBmGfUeAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUIInAAAAgxI8AQAAGJTgCQAAwKAETwAAAAYleAIAADAowRMAAIBBCZ4AAAAMSvAEAABgUEumM3JVXZvk9iT3JrmntbZiJgoFAADAwjGt4Nl7dmvtlhmYDgAAAAuQprYAAAAMarrBsyU5u6ourqojZqJAAAAALCzTbWr7zNbaD6rqV5N8oaq+2Vo7f3SAPpAekSSPecxjpjk7ABjO8qPPnOsirNG1xx4010UAgLU2rRrP1toP+v83JflMkr0mGOb41tqK1tqKZcuWTWd2AAAAzEPrHDyr6uFVtelYd5IDk1wxUwUDAABgYZhOU9utknymqsam8/HW2udmpFQAAAAsGOscPFtr30my2wyWBQAAgAXIz6kAAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwKMETAACAQQmeAAAADErwBAAAYFBL5roAAAAw05YffeZcF2G1rj32oLkuAswqNZ4AAAAMSvAEAABgUIInAAAAg5rX93iu7233E+33AQAA1HgCAAAwKMETAACAQQmeAAAADErwBAAAYFCCJwAAAIOa10+1BeAB6/uTvj3lGwAWLzWeAAAADErwBAAAYFCCJwAAAIMSPAEAABiU4AkAAMCgBE8AAAAGJXgCAAAwqGkFz6p6TlV9q6q+XVVHz1ShAAAAWDjWOXhW1QZJPpDkuUmelOTQqnrSTBUMAACAhWE6NZ57Jfl2a+07rbVfJPlEkufPTLEAAABYKKYTPLdJ8v2R19f3/QAAAOB+1VpbtxGrDk7ynNbaq/rXL0vyG62114wb7ogkR/Qvd0ryrXUv7qzYMsktc12Iec4ynD7LcPosw+mzDKfPMpwZluP0WYbTZxlOn2U4ffNhGW7fWls2vueSaUzwB0m2G3m9bd/vQVprxyc5fhrzmVVVtbK1tmKuyzGfWYbTZxlOn2U4fZbh9FmGM8NynD7LcPosw+mzDKdvPi/D6TS1vSjJjlW1Q1U9NMlLknx2ZooFAADAQrHONZ6ttXuq6jVJPp9kgyQnttaunLGSAQAAsCBMp6ltWmtnJTlrhsqyvpg3zYLXY5bh9FmG02cZTp9lOH2W4cywHKfPMpw+y3D6LMPpm7fLcJ0fLgQAAABTMZ17PAEAAGCNFlXwrKp7q2pVVV1ZVV+vqjdU1UOq6rf6/quq6o6q+lbf/c9VtUlV/WNV/VdVXVxV51XVb8z1Z5mqqtqqqj5eVd/py39hVb1w4HmuqKr3TWP8a6vqtJHXB1fVSX334VV188j3+KmqetgMFHtWVNUdI92/XVVXV9X2VXVMVf2sqn51kmEfVVWfGFkPz6qqJ8x2+dcXVfXm/vu/rF8X/rKq/mbcMLtX1VV997VVdcG491dV1RWzWe6hjezjvl5Vl1TV0weYx7S274WkqratqtOr6pp+H/v+qtqoqvapqp9U1aX98eT8qnreuHGX9PuyY+eg3HeseahJx/1IVT1pNe8fXlWPXsvhx/bp36yq169r2YZQVV+d6zJMxUTfaX9c+UG/bK+pqk+P/y76/WSrqufMXmnnh/64sWXfPei+tareNJPTmw2jy2ea0xndB6yqqn+eifJNMq+j1vdzxn57/JeR12PHijP614dX1fsnGO/aqrq8Py86u6oeNZvlnqpFFTyT/Ly1tntr7clJDkjy3CR/2Vr7fN9/9yQrk7y0f/3yJB9J8qMkO7bWnpLkFel+P2e9V1WV5N+SnN9ae2xf/pek++mbwbTWVrbWXjvNyTxlNScrp458j79Icsg05zXrqmq/JO9L8tzW2nV971uSvGGCYSvJZ5Kc11p7XP89/nmSrWarvOuTqnpakucl+fXW2q5J9k9ybn55PXhJklNGXm9aVdv109h5Nso6B8b2cbulW0f+Zk0jrK0Z2r7nvX67/HSSf2ut7ZhkxyQbJ/nbfpALWmt7tNZ2SvLaJO/vt/sxByS5OsmL+2nNC621V7XWvrGaQQ5Pcn/wnMLwSb9PT/KMJG8e206no6qm9QyLMa21Gb94M8ve2+8TdkxyapIvVdXob+sdmuTL/f8FbxrrxdD71lkNntVZnzLA2Hnd2Ln3Gq3jZzgqyXodPJPcmeTXqmrj/vUBmeDnKifx7P68aGVmeZ2aqvVppZtVrbWbkhyR5DWTHfSr6nFJfiPJX7TW7uvH+25r7czZK+m07JvkF621D4/1aK1d11o7rqqWV9UF/ZW7+6/e9Vfqzxgbvr+Cf3jffWxVfaO/mvLuvt+Lq+qK/irg+eOnUVV7VVfLemlVfbWqdur7H95fff1cfyV27GRtzHuSvHl1H64/gDw8yY+nt5hmV1U9K8kJSZ7XWvuvkbdOTHJIVW0+bpRnJ7l73Pf49dbaBVmctk5yS2vtv5OktXZLa+38JD+uB7dG+L08OHj+ax4Ip4eOe28hekT6baO6lhvn9Nv65VX1/LGBquot1dXKfbmqTqmqP+3771kP1Ci/q/ra4XHb9zFVdWJ1LUG+U1WvXdN0F5B9k9zVWvunJGmt3Zvk9UlenmST0QFba6uSvD3Ja0Z6H5rkH5J8L8nTZqG8v6Q/cXtXvw+/vKoO6fs/pKo+WF0N5Beqa2FxcP/eedXVem9QVSeNjPv6fpgVSU7u15uNx4bvx31Ovw5+varOGV+e1tqtSb6dbhtPVf1+Vf1nP61/rKoN+v6vrK61yH9W1QnVX/3vy/Phqvpakr+tqsf1x5iL++PdE/vhJjpuPXlkXpdV1Y59/zvWsKz26T/jp/rldXLV+nkhobV2apKzk/yP5P6LJy9Od7HggKpaOnelmxkT7Xf67+fvq2plktdV1e9U1df685IvVtVW/bhbVFdTdGVVfSTJZN/j6L51svVisv5bV9cCYlX/3t7VtXrYuO938oDLZnm/bP45yRVJPlpVK/vP+7aR4a6tqrfVA8eLse1m0uVTVf+z/zxXVNVRI/P7Zr9dXt1vG/tX1VeqO+/baw3lnWyao59hu6p6Y1Vd1G+3b+uHe3hVndlv41dU1SHVHZ8eneTcqjp3BhftEM5KclDfvS7nK+cnefyMlmimtNYWzV+SOybod1uSrUZen5dkRd/9u0k+M9flnsbnfW26q50TvfewJEv77h2TrOy790lyxshw7093UNoiybfywAOpfqX/f3mSbcb1u38a6XbQS/ru/ZOc1ncfnuQ7STZLsjTJdUm269+7Nl1t3lXpNpyDk5w0Mt7NSVYluTHJBUk2mOtlvRbfyd3patB3Hdf/mCR/muStSd42ur6u7ntcjH/pTupXpast+mCS3+z7/+nYckry1LF1emSd2inJV/vXlyZ5UpIr5vrzzPCyubdfNt9M8pMkT+n7L0nyiL57y3Qn95Vkz374pUk2TXJNkj/th7siydP67mPHltW47fuYJF9NslE/3VuTbLi66S6Uv8m2y37dOioj+9G+/+5Jruq7lyb5v+lqSI9Ictwsl31s3/KiJF9I95NoW6ULwVv3+9yz0l2cflS6k+yD+3HOSxcun5LkCyPT/JXR90f6jw2/LMn3k+zQ99+8/394kvf33Y8ZWW92TvLvSTbs3/tgulD/6H573rxf1y4YGf+kJGekPyYkOSdda6Wku4j8pb57ouPWcelaOyXJQ5NsPMVltU+6bW3bfnldmOSZc7A+TnR+c8z47a5fNz/Udz8jyTl998eTvGiut6tpLoMJ9zv9OvjBkeEemQfOZV6V5D199/uSvLXvPihJS7Jl/3qyfetk68Vk/d+Q5M39uBsk2XSy72+A5bM8yX1Jntq/3nykHOelPy/pt68j++4/TvKR1S2fdPuCy9NVBGyS5Moke/TzuyfJLv22cXG6C+yV5PnpWoskDz6vW5WuZeHqpjn6GQ5M94TX6udxRpJn9cv/hJHPvtnIZ9tyrtfVNXxPdyTZNcmn+nV5VR583D08/T5v3Hj3f7Z05+7vnOvPMtHfoq3xXIyq6gP91Z+L0h2wT6iqy5N8Mt1J+Or8JMld6a6Q/T9Jftb3/0qSk6rq/0238xpvsySfrK625L1Jnjzy3jmttZ+01u5K8o0k24+8d2+Sd6Vr0jLeWLOsR6XbMb1xDWVfn9yd7kT9lZO8/74kh1XVprNXpPmltXZHuoPSEekOVqdWVyt/apKDq2t6M76ZbdKFoh9X1UvSXdT4WRaeseZgT0zynCT/3NdqVJK/rqrLknwxyTbpToaekeT01tpdrbXb053op6p+Jd0J0YX9dD++mnme2Vr779baLUluWt10F7nR2pPnJTm3tfbzJKcleUH1tXmz7JlJTmmt3dtauzHJ/0l38v7MJJ9srd3XWvthuqbs430nyWOr6rjq7g/86Rrm9dR0t318N0laaz8aee+Qft38drqAcFeS/dJt5xdV1ar+9WOT7JXk/7TWftRauzvd8WvUJ1tr91bVJkmenu74syrJP6avSc3Ex60Lk7ypqv4syfb9dzOVZZUk/9lau751LaNWpTs5Xl+NroeHJvlE3/2JzP/mtqvb75w60r1tks/35z9vzAPnJc9K8i9J0rqWbaOtqSbbt65uG5qo/0VJXlFVxyTZpS/nbLqutfYffffvVdUl6S6WPTkPPg/8dP//4jywPk+2fJ6ZrpLmzv74/Okke/fvfbe1dnm/bVyZ7ryvpTt3G5tu8uCmtv+0hmmOfoYD+79Lk1yS5InpKlMuT1eL/86q2ru19pO1XlJzqLV2Wbrlc2jW7mcrz+33d4/IALfazIRFHTyr6rHpAs5NkwxyZZLd5uiEYCZcmeTXx1601v4k3cF7WbomYTcm2S3d1eiH9oPdkwevF0v7ce9Jd8D/VLqTps/1/f8oyV8k2S7JxVW1xbgy/FW6E6xfS/I7Y9Pr/fdI97355d+V/d/pdnQT3u/T77z+vR9mvrgvXRPQvWqChwm01m5Ld5L/JyO9r0x3AkavP5if11r7y3TNF1/UWvt+ku8m+c10VztPnWDUU5N8IAu/mW360Lhluu39pf3/p/QXbW7Mg7fF6VjTdrxQfSPjtsuqekS6C2LfmmD4PdJd8Ei6k4n9q+radCd2W6RrujtvtNZ+nO74cV6SP0r3PIR1dWrr7kt6epJjq3soRiX52MjJ6E6ttWOmMK07+/8PSXLbyPi7t9Z27sv+S8et1trH07Vy+nmSs6pqbb6P+bQN7JHkqv685kVJ3tqvh8clec4Cvuh550j3celqjHZJ8odZy33huH3rWmndbSHPSnfP3klVNaX7GWfQnUlSVTukqw3er9/2zszE52fTXZ9Ht437Rl7fN43pjn6XleRvRrbxx7fWPtpauzrd+e/lSf6/qnrrOs5rLn02ybuzducrz+6Xw8v788n1zqINntXdXP/hdDufCX/MtHX3361M8rb+ytZY+/KDJhp+PfSlJEur6tUj/cZuqt4syQ39VaiX5YGrvtcleVJ1T2b8lXRBNf3V481aa2elC6279f0f11r7Wmvtrelqn8aHxM3ywE3Rh69N4fur2e/t5zeZZyb5r9W8v95prf0sXTOVl1bVRDWff5fuYDi2U/5Sko2q6oixAapq16rae4JxF7yq2qn6+696u6dbb5NuB/3eJN9prV0/weifSffwl88PWsj1QH9fzgbpano3S3JTa+3uqnp2Hmhd8JUkv1NVS/tt/HnJ/RdAbq8H7pl9yVrOfsLpLjDnJHnY2IljfyL/nnRNnB5UW1ZVuyZ5S5IP9OF07ySPaa0tb60tT3ehaS5qmy5IV9u4QX9MfFaS/0z3/b2ouns9t0rXzOtBqnua5UNaa6elC3FjFzlvT9fMcbz/SPKs/oQ39cv3sqe1tjLdBcfXpVu+B1f/pO+q2ryqtk9XY/SbVfXI6u7zf9FEH6y19tMk362qF/fjV1VNetzqL0R/p7X2viSnp2vqNpVlNW9U1YvS1Q6dku7Yfllrbbt+Pdw+Xe37oE+9H9hU9zuj5yWHjfQ/Pw/c//rcdE1yf8m4fetk68WE/ft1+MbW2gnpLtaMbTd3V9WG6/Kh19Ej0gW4n/Tb+HOnMM5ky+eCdK02HlZVD0+3Dk33GRRTnebnk/xB/32nqrapql+t7snaP2ut/Uu61nNr2j+tj05Md+vV5XNdkJm0Pl+VG8LGfRX0hulq9v53upP81XlVupOJb1fVz9M9eXReNO1srbWqekGS91bV/0p3gL0zyZ+la5JwWn/S9Lm+f1pr36+qf013f9d30zVfSLoN9fTqHj5QSf5n3/9dfQiodCcKX09X4zTmb5N8rKr+It0VtbX10XQnNaMOqapnprtwcn3WMtCuD1prP+qbp51fVTePe++WqvpM+sDdf48vTPL3fTOwu9K15T9qdku93tgkyXH9hZF70jXPGwvln0zXXPnIiUbsmzW9M0lq/Xz+x3SN7eOSbps8rG92eHKSf++blq1Md59SWmsXVdVnk1yWrhb08nTN6pOuOfgJVXVfumZiU26qtIbpLggj2+UHquot6Wo/Tm2tvaOq9kmyd1Vdmu5i301JXttaO6eqDkt3r+FoTcDp6R6Gs9G4/kP7TLoHG3093f1a/6u19sPqfs5qv3S1ut9Pd7wY//1tk+Sf6oGnSo7dFnFSkg/3x8v7H5rUWru5v3j26X6cm9I9rXG8d/bz++t0+/6z++HvTvInrbX/qKq/Tndy/6M8cM/dRF6a5EP98WfDdM1Jv56Jj1t/luRlVXV3kh/285/KsnriJPOebQ+rqtGLbWPnNq+vqt9Pd6/cFUn27b+LQ9N9plGnJXl1ksF+zmJIa7HfOSZdE+wfp7uwu0Pf/21JTqmqK9PdEvO9kXEm27dOtl5M1v+wJG/s17M70t23nHT3KV5WVZe01l463WWxJq21r/f7p2+m28a/MoXRJlw+rbVLqvvJu7ELMR9prV1aVcunUb4pTbO1dnZ1T6m/sD+m35Hk99M9H+Rd/fHr7nTrddIt589V1f9trT17Xcs3G/qL55P9dNnh/fn9mKcOX6KZMXZzNQCLUFVt0lq7o7rfNjs/yRH9QX+T/t6aVNXRSbZurb1uutMd5EOsB6p7MvgpSV64ED7nyPe3RbqTv2e07n7POTdStiXpwtOJrbXxIYpFaLHtd2C+WWw1ngA82PHV/Wbu0nT31I2dpB1UVX+e7jhxXda+ZcFk012QWmtfzYMfkDbfndG3Knhokr9aX0Jn75iq2j/dunV2ut+rhmSR7XdgvlHjCQAAwKAW7cOFAAAAmB2CJwAAAIMSPAEAABiU4AnAolVVj6qqT1TVf1XVxVV1VlU9oaqumMF5vL1/GE6qau+qurKqVvW/OfepmZoPAKzPPFwIgEWpuh9++2q6p19+uO+3W7ofV/9Qa+3XBpjnh5N8uf9h85mc7gattXtncpoAMJPUeAKwWD07yd1joTPpflg93Q+qJ0mqanlVXVBVl/R/T+/7b11V5/c1l1f0NZkbVNVJ/evLq+r1/bAnVdXBVfWqJL+X5K+q6uR+2lf0w2xQVe+qqouq6rKq+sO+/z5VdcZIed5fVYf33ddW1Tur6pIkLx56YQHAdPgdTwAWq19LcvEahrkpyQGttbuqasckpyRZkeR/JPl8a+0dVbVBkocl2T3JNmM1pf3vYN6vtfaRqnpmkjNaa5+qquUjb78yyU9aa3tW1UZJvlJVZ0/hM9zaWvv1KQwHAHNK8ASAyW2Y5P1VtXuSe5M8oe9/UZITq2rDJP/WWltVVd9J8tiqOi7JmUmmEhzHHJhk16o6uH+9WZIdk/xiDeOduhbzAIA5o6ktAIvVlUmesoZhXp/kxiS7pavpfGiStNbOT/KsJD9IclJVvby19uN+uPOS/FGSj6xFWSrJka213fu/HVprZye5Jw8+Vi8dN96dazEPAJgzgicAi9WXkmxUVUeM9aiqXZNsNzLMZkluaK3dl+RlSTboh9s+yY2ttRPSBcxfr6otkzyktXZakr9IsjZNYD+f5NV9DWr6J+s+PMl1SZ5UVRv1TXf3W7ePCgBzS1NbABal1lqrqhcm+fuq+rMkdyW5NslRI4N9MMlpVfXyJJ/LAzWM+yR5Y1XdneSOJC9Psk2Sf6qqsYu6f74WxflIkuVJLumftntzkhe01r5fVf+a5Iok301y6Vp+TABYL/g5FQAAAAalqS0AAACDEjwBAAAYlOAJAADAoARPAAAABiV4AgAAMCjBEwAAgEEJngAAAAxK8AQAAGBQ/z91V3SulrHS2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = prepare_UNSW_NB15(dfad2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "score, clfName = all_classifier_score(X_train, X_test, y_train, y_test)\n",
    "plotClassifieurBarScore(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
