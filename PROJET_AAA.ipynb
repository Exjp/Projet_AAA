{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Adversarial Machine Learning in Network Intrusion Detection Systems"},{"metadata":{},"cell_type":"markdown","source":"L'article utilise comme dataset NSL-KDD et UNSW-NB15, qui représente des flux de données bruts. Ces flux sont un mélanges différent type de trafic mais aussi un mélange  de bonnes et malicieuses données. Le but étant d'altérer ces paquets de données pour pouvoir passer les modèles de machine learning de sécurité tout en gardant en gardant les flux de donnée fonctionnels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\n\n\ndf = pd.read_csv(\"KDDTest+.csv\")\ndf.info()","execution_count":1,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22544 entries, 0 to 22543\nData columns (total 42 columns):\n #   Column                       Non-Null Count  Dtype  \n---  ------                       --------------  -----  \n 0   duration                     22544 non-null  int64  \n 1   protocol_type                22544 non-null  object \n 2   service                      22544 non-null  object \n 3   flag                         22544 non-null  object \n 4   src_bytes                    22544 non-null  int64  \n 5   dst_bytes                    22544 non-null  int64  \n 6   land                         22544 non-null  int64  \n 7   wrong_fragment               22544 non-null  int64  \n 8   urgent                       22544 non-null  int64  \n 9   hot                          22544 non-null  int64  \n 10  num_failed_logins            22544 non-null  int64  \n 11  logged_in                    22544 non-null  int64  \n 12  num_compromised              22544 non-null  int64  \n 13  root_shell                   22544 non-null  int64  \n 14  su_attempted                 22544 non-null  int64  \n 15  num_root                     22544 non-null  int64  \n 16  num_file_creations           22544 non-null  int64  \n 17  num_shells                   22544 non-null  int64  \n 18  num_access_files             22544 non-null  int64  \n 19  num_outbound_cmds            22544 non-null  int64  \n 20  is_host_login                22544 non-null  int64  \n 21  is_guest_login               22544 non-null  int64  \n 22  count                        22544 non-null  int64  \n 23  srv_count                    22544 non-null  int64  \n 24  serror_rate                  22544 non-null  float64\n 25  srv_serror_rate              22544 non-null  float64\n 26  rerror_rate                  22544 non-null  float64\n 27  srv_rerror_rate              22544 non-null  float64\n 28  same_srv_rate                22544 non-null  float64\n 29  diff_srv_rate                22544 non-null  float64\n 30  srv_diff_host_rate           22544 non-null  float64\n 31  dst_host_count               22544 non-null  int64  \n 32  dst_host_srv_count           22544 non-null  int64  \n 33  dst_host_same_srv_rate       22544 non-null  float64\n 34  dst_host_diff_srv_rate       22544 non-null  float64\n 35  dst_host_same_src_port_rate  22544 non-null  float64\n 36  dst_host_srv_diff_host_rate  22544 non-null  float64\n 37  dst_host_serror_rate         22544 non-null  float64\n 38  dst_host_srv_serror_rate     22544 non-null  float64\n 39  dst_host_rerror_rate         22544 non-null  float64\n 40  dst_host_srv_rerror_rate     22544 non-null  float64\n 41  class                        22544 non-null  object \ndtypes: float64(15), int64(23), object(4)\nmemory usage: 7.2+ MB\n","name":"stdout"}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df2 = pd.read_csv(\"UNSW_NB15_training-set.csv\")\ndf2.info()","execution_count":2,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 175341 entries, 0 to 175340\nData columns (total 45 columns):\n #   Column             Non-Null Count   Dtype  \n---  ------             --------------   -----  \n 0   id                 175341 non-null  int64  \n 1   dur                175341 non-null  float64\n 2   proto              175341 non-null  object \n 3   service            175341 non-null  object \n 4   state              175341 non-null  object \n 5   spkts              175341 non-null  int64  \n 6   dpkts              175341 non-null  int64  \n 7   sbytes             175341 non-null  int64  \n 8   dbytes             175341 non-null  int64  \n 9   rate               175341 non-null  float64\n 10  sttl               175341 non-null  int64  \n 11  dttl               175341 non-null  int64  \n 12  sload              175341 non-null  float64\n 13  dload              175341 non-null  float64\n 14  sloss              175341 non-null  int64  \n 15  dloss              175341 non-null  int64  \n 16  sinpkt             175341 non-null  float64\n 17  dinpkt             175341 non-null  float64\n 18  sjit               175341 non-null  float64\n 19  djit               175341 non-null  float64\n 20  swin               175341 non-null  int64  \n 21  stcpb              175341 non-null  int64  \n 22  dtcpb              175341 non-null  int64  \n 23  dwin               175341 non-null  int64  \n 24  tcprtt             175341 non-null  float64\n 25  synack             175341 non-null  float64\n 26  ackdat             175341 non-null  float64\n 27  smean              175341 non-null  int64  \n 28  dmean              175341 non-null  int64  \n 29  trans_depth        175341 non-null  int64  \n 30  response_body_len  175341 non-null  int64  \n 31  ct_srv_src         175341 non-null  int64  \n 32  ct_state_ttl       175341 non-null  int64  \n 33  ct_dst_ltm         175341 non-null  int64  \n 34  ct_src_dport_ltm   175341 non-null  int64  \n 35  ct_dst_sport_ltm   175341 non-null  int64  \n 36  ct_dst_src_ltm     175341 non-null  int64  \n 37  is_ftp_login       175341 non-null  int64  \n 38  ct_ftp_cmd         175341 non-null  int64  \n 39  ct_flw_http_mthd   175341 non-null  int64  \n 40  ct_src_ltm         175341 non-null  int64  \n 41  ct_srv_dst         175341 non-null  int64  \n 42  is_sm_ips_ports    175341 non-null  int64  \n 43  attack_cat         175341 non-null  object \n 44  label              175341 non-null  int64  \ndtypes: float64(11), int64(30), object(4)\nmemory usage: 60.2+ MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Genetic Algorithm"},{"metadata":{},"cell_type":"markdown","source":"Le *Genetic algorithm* (GA) est un algorithme qui se base sur la génétique et la sélection naturelle. Le GA utilise une forme de données appelées chromosome : ces données (vecteurs) sont formées et les valeurs de différentes features sont destinées à évoluer comme pour un gêne d'être vivant.\nCertaines *features* sont inchangeable du à leur importance dans le bon fonctionnement des paquets.\n\n- En premier lieu, on va créer une population de 100 chromosomes, dont leurs données seront générer aléatoirement, séparées en 2 parties, les données que l'ont peu modifier et les autres. (pour les opérations qui suivent, on va utiliser la partie que l'on peut modifier)\n- on va ensuite utiliser l'opération de *cross over*. On choisi aléatoirement 2 chromosomes que l'on va séparer l'un de 25% de ses features et donc, de l'autre des 75% autres, que l'on va échanger pour créer 2 nouveau chromosomes. Ces nouveaux chromosomes seront ensuite insérés dans la population.\n- Enfin, on va utiliser l'opération de *mutation* sur la partie que l'on peut modifier, qui consiste à choisir de manière aléatoire un chromosome et de même pour une de ses features. Toute cette opération est exécutée \n- On regroupe les 2 parties des chromosomes.\n- Finalement, on va utiliser une *fitness function* pour évaluer chaque chromosome, s'ils sont bénins (seuil à 99,99%) ou non. On retiendra le meilleur chromosome toutes les 5 générations.\n- On répète tout ce processus 100 fois\nOn obtient alors un nouveau dataset ne contenant que des vecteurs \"malsains\" qui seront testés dans la partie expérience."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install geneticalgorithm","execution_count":3,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: geneticalgorithm in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.0.1)\r\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from geneticalgorithm) (1.19.4)\r\nRequirement already satisfied: func-timeout in /srv/conda/envs/notebook/lib/python3.6/site-packages (from geneticalgorithm) (4.3.5)\r\n","name":"stdout"}]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import sklearn\nimport numpy as np\nimport random\nfrom random import randint\nfrom geneticalgorithm import geneticalgorithm as ga\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn import ensemble\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n#Creation of initial population\nlistProtocols = [\"tcp\",\"udp\",\"icmp\"]\nlistData = [\"private\",\"ftp_data\",\"eco_i\",\"telnet\",\"http\",\"smtp\",\"imap4\",\"systat\",\"pop3\",\"domain_u\",\"whois\",\"netbios_dgm\"]\nlistSF = [\"SF\",\"S0\",\"REJ\",\"RSTR\"]\n#cell 3 = 62825648 - 0\n# 24 à 30 et 33 à 40 entre 0 et 1\n# 4 à 23 , 31- 32,  entre 0 et 20\n#derniere = 0 - 21\n\n# creation of artifical dataset\ndef initialPopulation(df, nbChrom):\n    initPopulation = []\n    for i in range (nbChrom):\n        chrom = []\n        for j in range (len(df.columns)):\n            if(j == 0) :\n                chrom.insert(j,random.randint(0, 2))\n            if(j == 1) :\n                chrom.insert(j,random.randint(0, 11))\n            if(j == 2) :\n                chrom.insert(j,randint(0, 62825648))\n            if((j >= 24 and j <= 30) or (j >= 33 and j <= 40)) :\n                chrom.insert(j,random.uniform(0, 1))\n            if((j >= 4 and j <= 23) or j == 32 or j ==33) :\n                chrom.insert(j,random.randint(0, 62825648))\n            if(j == 41) :\n                chrom.insert(j,random.randint(0,21))\n        initPopulation.append(chrom)\n    return initPopulation\n\n\n\n\ndimDF = len(df.columns)\ndimDF2 = len(df2.columns)\n\n#We have to reduce to a minimum data size otherwise our computers can not run the fit after...\ndfCopy = df.copy()\ndfCopy = dfCopy.head(1000)\n\n# preparation of the data : transforming string input to int \nLE = LabelEncoder()\ndfCopy['protocol_type'] = LE.fit_transform(dfCopy['protocol_type'])\ndfCopy['service'] = LE.fit_transform(dfCopy['service'])\ndfCopy['flag'] = LE.fit_transform(dfCopy['flag'])\ndfCopy['class'] = LE.fit_transform(dfCopy['class'])\ndfCopy.head()\n\ny = dfCopy['class']\ndfCopy.drop(['class'], axis='columns', inplace=True)\nX = dfCopy\n\n","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npopulation = initialPopulation(dfCopy,100)\n#fitnessScore(population,)\n#population","execution_count":5,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Parameters given in the section 4.3 of the article\nsvm = svm.SVC(C = 220, gamma = 0.01, probability = True, tol = 0.001)\n#  decision trees\ndectr = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 4, \\\n                                    min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n#naive bayes\nnb = GaussianNB()\n#k nearest neighbors\nknn = KNeighborsClassifier(n_neighbors = 3, algorithm ='auto')\n\nestimators = [ ('svm', svm), ('dt', dectr), ('nb', nb), ('knn', knn)]\nmodel = ensemble.VotingClassifier(estimators, voting='hard')\n\nfitModel = model.fit(X,y)\n\n\n# https://datascienceplus.com/genetic-algorithm-in-machine-learning-using-python/\n\ndef fitnessScore(population,yTest):\n    scores = []\n    for chromosome in population :\n        prediction = fitModel.predict(chromosome)\n        # We want the bad pred to be valued better\n        # 0 correspond to \"anomaly\" and 1 to \"normal\"\n        if(prediction == 0):\n            score = 1\n        else :\n            score = 0\n        scores.append(score)\n    scores, population = np.array(scores), np.array(population) \n    inds = np.argsort(scores)\n    return list(scores[inds][::-1]), list(population[inds,:][::-1])\n\n''' # TO ADAPT\ndef selection(pop_after_fit,n_parents):\n    population_nextgen = []\n    for i in range(n_parents):\n        population_nextgen.append(pop_after_fit[i])\n    return population_nextgen\n\ndef crossover(pop_after_sel):\n    population_nextgen=pop_after_sel\n    for i in range(len(pop_after_sel)):\n        child=pop_after_sel[i]\n        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n        population_nextgen.append(child)\n    return population_nextgen\n\ndef mutation(pop_after_cross,mutation_rate):\n    population_nextgen = []\n    for i in range(0,len(pop_after_cross)):\n        chromosome = pop_after_cross[i]\n        for j in range(len(chromosome)):\n            if random.random() < mutation_rate:\n                chromosome[j]= not chromosome[j]\n        population_nextgen.append(chromosome)\n    #print(population_nextgen)\n    return population_nextgen\n\ndef generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n                                   X_test, y_train, y_test):\n    best_chromo= []\n    best_score= []\n    population_nextgen=initilization_of_population(size,n_feat)\n    for i in range(n_gen):\n        scores, pop_after_fit = fitness_score(population_nextgen)\n        print(scores[:2])\n        pop_after_sel = selection(pop_after_fit,n_parents)\n        pop_after_cross = crossover(pop_after_sel)\n        population_nextgen = mutation(pop_after_cross,mutation_rate)\n        best_chromo.append(pop_after_fit[0])\n        best_score.append(scores[0])\n    return best_chromo,best_score\n'''","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"' # TO ADAPT\\ndef selection(pop_after_fit,n_parents):\\n    population_nextgen = []\\n    for i in range(n_parents):\\n        population_nextgen.append(pop_after_fit[i])\\n    return population_nextgen\\n\\ndef crossover(pop_after_sel):\\n    population_nextgen=pop_after_sel\\n    for i in range(len(pop_after_sel)):\\n        child=pop_after_sel[i]\\n        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\\n        population_nextgen.append(child)\\n    return population_nextgen\\n\\ndef mutation(pop_after_cross,mutation_rate):\\n    population_nextgen = []\\n    for i in range(0,len(pop_after_cross)):\\n        chromosome = pop_after_cross[i]\\n        for j in range(len(chromosome)):\\n            if random.random() < mutation_rate:\\n                chromosome[j]= not chromosome[j]\\n        population_nextgen.append(chromosome)\\n    #print(population_nextgen)\\n    return population_nextgen\\n\\ndef generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\\n                                   X_test, y_train, y_test):\\n    best_chromo= []\\n    best_score= []\\n    population_nextgen=initilization_of_population(size,n_feat)\\n    for i in range(n_gen):\\n        scores, pop_after_fit = fitness_score(population_nextgen)\\n        print(scores[:2])\\n        pop_after_sel = selection(pop_after_fit,n_parents)\\n        pop_after_cross = crossover(pop_after_sel)\\n        population_nextgen = mutation(pop_after_cross,mutation_rate)\\n        best_chromo.append(pop_after_fit[0])\\n        best_score.append(scores[0])\\n    return best_chromo,best_score\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Le but est de maximiser le nombre de vecteurs malsain classés en sain."},{"metadata":{},"cell_type":"markdown","source":"# Particle Swarn Optimization"},{"metadata":{},"cell_type":"markdown","source":"L'idée du *Particle Swarm Optimization* est que pour chaque particule d'une nuée, on va essayer de trouver la meilleure position retournée par la *fitness function*. \n\n- D'abord, on doit créer la nuée de 200 particules. Les particules sont formées à partir d'une rangée du dataset et d'une partie généré aléatoirement, toujours avec les mêmes *features* que le dataset. Comme que pour le GA, on devrait prendre seulement la partie mutable des particules à modifier. Ensuites, pour chaque particules est attribué une vélocité de 0.7.\n- Pour chaque itération, on va évaluer chaque particule à partir de la *fitness function*. On met à jour la meilleur valeur de la fonction fitness possible de chaque particules (de 0.5 au départ) et de la meilleur valeur de la fonction fitness global (de 0.4 au départ)\n- On calcule en suite la distance entre la position de chaque particule et on récupère la meilleure valeur grâce à la fonction de fitness. On met à jour la vélocité et la position de chaque particule avec les valeurs précédentes.\n- On répète tout ce processus 100 fois au maximum ou s'il y a une amélioration de moins de 0.001%.\nLa nué de particules devient alors un nouveau dataset de vecteurs *malsains*"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyswarms","execution_count":7,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pyswarms in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.2.0)\nRequirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (5.3.1)\nRequirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (1.5.3)\nRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (4.54.0)\nRequirement already satisfied: attrs in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (19.3.0)\nRequirement already satisfied: matplotlib>=1.3.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (3.3.3)\nRequirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (1.19.4)\nRequirement already satisfied: future in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (0.18.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (1.3.1)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (2.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (8.0.1)\nRequirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=1.3.1->pyswarms) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.1)\nRequirement already satisfied: six in /srv/conda/envs/notebook/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=1.3.1->pyswarms) (1.15.0)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyswarms.single import GlobalBestPSO\nfrom pyswarms.utils.search.grid_search  import SearchBase\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef prepare_UNSW_NB15(data):\n    LE = LabelEncoder()\n    copy = data[data['label']==1]\n    copy = copy.drop(['label'], axis=1)\n    copy['proto'] = LE.fit_transform(copy['proto'])\n    copy['service'] = LE.fit_transform(copy['service'])\n    copy['state'] = LE.fit_transform(copy['state'])\n    copy['attack_cat'] = LE.fit_transform(copy['attack_cat'])\n    return copy\n\ndef voting_classifier(X_train, Y_train) :\n  sv = svm.SVC(gamma = 0.01, C = 220.0, tol = 0.01, probability = True)\n  dt = DecisionTreeClassifier(\n      criterion = \"entropy\", min_samples_split = 4, min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n  nb = GaussianNB()\n  kn = KNeighborsClassifier( n_neighbors = 3, algorithm = \"auto\")\n\n  vC = VotingClassifier (estimators=\n                         [('svm', sv), ('dt', dt), ('nb', nb), ('knn',kn)], voting='soft')\n  vC = vC.fit(X_train,Y_train)\n  return vC\n\ndef Pso (dim, data) :\n  option = {'c1': 0.5, 'c2': 0.4, 'w' : 0.7}\n  opt = GlobalBestPSO (n_particles=200, dimensions=dim, options=option)\n  return opt.optimize(prepare_UNSW_NB15, 100, None, True, data)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\n\n#df2prepare = Pso (len(copy.columns), df2)\n\n#X_train, X_test, y_train, y_test = train_test_split(df2prepare, test_size=0.2, random_state=300)\n\n#voting_classifier(X_train, y_train)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generative Adversarial Network"},{"metadata":{},"cell_type":"markdown","source":"Le *Generative Adversial Network* est une technique de deep learning qui va confronter deux réseaux de neuronnes. Ces deux réseaux de neuronnes correspondent l'un à la génération des vecteur *malicieux* et donc à ceux qu'elles permettent de tromper, l'autre est le discriminateur, qui lui va essayer de deviner si l'entrée est saine ou non.\nComme les méthodes précédentes, on va modifier d'abord la partie modifiable, c'est à dire non fixée des vecteurs. \n- Pour le générateur, on va sélectionner aléatoirement un vecteur \"malsain\". On va ajouter du bruit dans certaines des features choisies au hasard, toujours dans la partie modifiable du vecteur malicieux. Puis ce vecteur va rentrer dans le réseau neuronal du générateur avec comme sortie, un vecteur de la taille de la partie mutable du vecteur d'entrée. On recombine le vecteur de sortie avec la partie non changeable du vecteur de l'entrée, et on le donne en entrée du discriminateur. Les labels sortant de ce dernier permettrons d'améliorer le réseau de neurone du générateur.\n- Pour le discriminateur, on va faire la même chose avec mais un vecteur bénin.\n- Les deux réseaux de neurones utiliserons des sets de données d'entraînements.\n\n"},{"metadata":{},"cell_type":"markdown","source":"# EXPERIENCE"},{"metadata":{},"cell_type":"markdown","source":"Les expériences sont enfin réalisée sur les 11 modèles de classification en utilisant les sets produits précédemment. \nL'article ne mentionne pas les hyperparamètres à appliquer pour les 11 classifieurs. \nLes 11 classifieurs sont Support Vector Machine (SVM), Decision Trees (DT), Naive Bayes (NB), k Nearest Neighbors (KNN), Random Forest (RF), Multi-layer Perceptron (MLP), Gradient Boosting (GB), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Bagging (BAG)\nNous laisserons principalement les paramètres par défaut pour réaliser les expériences.\nLes résultats attendus pour ces expériences sont un taux élevé de mauvaise classification (dans le cas où nous arriverions à générer les adversairal datasets). \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classifier(clf, X_train, Y_train, X_test, Y_test):\n    model = clf();\n    model.fit(X_train, Y_train);\n    return model.score(X_test, Y_test)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def comparaison (clf, X_train, Y_train, X_test, Y_test) :\n    score = []\n    for i in clf :\n        score.append(classifier(i, X_train, Y_train, X_test, Y_test))\n    return score","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf2copy = df2.copy()\nlabel = df2.loc[:,('label')]\ndf2copy = df2copy.drop(['label'], axis=1)\ndf2copy['proto'] = LE.fit_transform(df2copy['proto'])\ndf2copy['service'] = LE.fit_transform(df2copy['service'])\ndf2copy['state'] = LE.fit_transform(df2copy['state'])\ndf2copy['attack_cat'] = LE.fit_transform(df2copy['attack_cat'])\n\nX_train, X_test, y_train, y_test = train_test_split(df2copy, label, test_size=0.5, random_state=500)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nclf = [DecisionTreeClassifier, GaussianNB, KNeighborsClassifier]\n\ncomparaison(clf, X_train, y_train, X_test, y_test)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[1.0, 0.8495169440293826, 0.8976856657275496]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=500)\ncomparaison(clf, X_train, y_train, X_test, y_test)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"[0.932, 0.784, 0.896]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}