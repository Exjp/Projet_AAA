{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Machine Learning in Network Intrusion Detection Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'article utilise comme dataset NSL-KDD et UNSW-NB15, qui représente des flux de données bruts. Ces flux sont un mélanges différent type de trafic mais aussi un mélange  de bonnes et malicieuses données. Le but étant d'altérer ces paquets de données pour pouvoir passer les modèles de machine learning de sécurité tout en gardant en gardant les flux de donnée fonctionnels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"KDDTest+.csv\")\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
    "#df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Genetic algorithm* (GA) est un algorithme qui se base sur la génétique et la sélection naturelle. Le GA utilise une forme de données appelées chromosome : ces données (vecteurs) sont formées et les valeurs de différentes features sont destinées à évoluer comme pour un gêne d'être vivant.\n",
    "Certaines *features* sont inchangeable du à leur importance dans le bon fonctionnement des paquets.\n",
    "\n",
    "- En premier lieu, on va créer une population de 100 chromosomes, dont leurs données seront générer aléatoirement, séparées en 2 parties, les données que l'ont peu modifier et les autres. (pour les opérations qui suivent, on va utiliser la partie que l'on peut modifier)\n",
    "- on va ensuite utiliser l'opération de *cross over*. On choisi aléatoirement 2 chromosomes que l'on va séparer l'un de 25% de ses features et donc, de l'autre des 75% autres, que l'on va échanger pour créer 2 nouveau chromosomes. Ces nouveaux chromosomes seront ensuite insérés dans la population.\n",
    "- Enfin, on va utiliser l'opération de *mutation* sur la partie que l'on peut modifier, qui consiste à choisir de manière aléatoire un chromosome et de même pour une de ses features. Toute cette opération est exécutée \n",
    "- On regroupe les 2 parties des chromosomes.\n",
    "- Finalement, on va utiliser une *fitness function* pour évaluer chaque chromosome, s'ils sont bénins (seuil à 99,99%) ou non. On retiendra le meilleur chromosome toutes les 5 générations.\n",
    "- On répète tout ce processus 100 fois\n",
    "On obtient alors un nouveau dataset ne contenant que des vecteurs \"malsains\" qui seront testés dans la partie expérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geneticalgorithm in /home/jp/.local/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy in /home/jp/.local/lib/python3.8/site-packages (from geneticalgorithm) (1.19.2)\n",
      "Requirement already satisfied: func-timeout in /home/jp/.local/lib/python3.8/site-packages (from geneticalgorithm) (4.3.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install geneticalgorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Creation of initial population\n",
    "listProtocols = [\"tcp\",\"udp\",\"icmp\"]\n",
    "listData = [\"private\",\"ftp_data\",\"eco_i\",\"telnet\",\"http\",\"smtp\",\"imap4\",\"systat\",\"pop3\",\"domain_u\",\"whois\",\"netbios_dgm\"]\n",
    "listSF = [\"SF\",\"S0\",\"REJ\",\"RSTR\"]\n",
    "#cell 3 = 62825648 - 0\n",
    "# 24 à 30 et 33 à 40 entre 0 et 1\n",
    "# 4 à 23 , 31- 32,  entre 0 et 20\n",
    "#derniere = 0 - 21\n",
    "\n",
    "# creation of artifical dataset\n",
    "def initialPopulation(df, nbChrom):\n",
    "    initPopulation = []\n",
    "    for i in range (nbChrom):\n",
    "        chrom = []\n",
    "        for j in range (len(df.columns)):\n",
    "            if(j == 0) :\n",
    "                chrom.insert(j,random.randint(0, 2))\n",
    "            if(j == 1) :\n",
    "                chrom.insert(j,random.randint(0, 11))\n",
    "            if(j == 2) :\n",
    "                chrom.insert(j,randint(0, 62825648))\n",
    "            if((j >= 24 and j <= 30) or (j >= 33 and j <= 40)) :\n",
    "                chrom.insert(j,random.uniform(0, 1))\n",
    "            if((j >= 4 and j <= 23) or j == 32 or j ==33) :\n",
    "                chrom.insert(j,random.randint(0, 62825648))\n",
    "            if(j == 41) :\n",
    "                chrom.insert(j,random.randint(0,21))\n",
    "        initPopulation.append(chrom)\n",
    "    return initPopulation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimDF = len(df.columns)\n",
    "dimDF2 = len(df2.columns)\n",
    "\n",
    "#We have to reduce to a minimum data size otherwise our computers can not run the fit after...\n",
    "dfCopy = df.copy()\n",
    "dfCopy = dfCopy.head(1000)\n",
    "\n",
    "# preparation of the data : transforming string input to int \n",
    "LE = LabelEncoder()\n",
    "dfCopy['protocol_type'] = LE.fit_transform(dfCopy['protocol_type'])\n",
    "dfCopy['service'] = LE.fit_transform(dfCopy['service'])\n",
    "dfCopy['flag'] = LE.fit_transform(dfCopy['flag'])\n",
    "dfCopy['class'] = LE.fit_transform(dfCopy['class'])\n",
    "dfCopy.head()\n",
    "\n",
    "y = dfCopy['class']\n",
    "dfCopy.drop(['class'], axis='columns', inplace=True)\n",
    "X = dfCopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "population = initialPopulation(dfCopy,100)\n",
    "#fitnessScore(population,)\n",
    "#population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TO ADAPT\\ndef selection(pop_after_fit,n_parents):\\n    population_nextgen = []\\n    for i in range(n_parents):\\n        population_nextgen.append(pop_after_fit[i])\\n    return population_nextgen\\n\\ndef crossover(pop_after_sel):\\n    population_nextgen=pop_after_sel\\n    for i in range(len(pop_after_sel)):\\n        child=pop_after_sel[i]\\n        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\\n        population_nextgen.append(child)\\n    return population_nextgen\\n\\ndef mutation(pop_after_cross,mutation_rate):\\n    population_nextgen = []\\n    for i in range(0,len(pop_after_cross)):\\n        chromosome = pop_after_cross[i]\\n        for j in range(len(chromosome)):\\n            if random.random() < mutation_rate:\\n                chromosome[j]= not chromosome[j]\\n        population_nextgen.append(chromosome)\\n    #print(population_nextgen)\\n    return population_nextgen\\n\\ndef generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\\n                                   X_test, y_train, y_test):\\n    best_chromo= []\\n    best_score= []\\n    population_nextgen=initilization_of_population(size,n_feat)\\n    for i in range(n_gen):\\n        scores, pop_after_fit = fitness_score(population_nextgen)\\n        print(scores[:2])\\n        pop_after_sel = selection(pop_after_fit,n_parents)\\n        pop_after_cross = crossover(pop_after_sel)\\n        population_nextgen = mutation(pop_after_cross,mutation_rate)\\n        best_chromo.append(pop_after_fit[0])\\n        best_score.append(scores[0])\\n    return best_chromo,best_score\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters given in the section 4.3 of the article\n",
    "svm = svm.SVC(C = 220, gamma = 0.01, probability = True, tol = 0.001)\n",
    "#  decision trees\n",
    "dectr = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 4, \\\n",
    "                                    min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "#naive bayes\n",
    "nb = GaussianNB()\n",
    "#k nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, algorithm ='auto')\n",
    "\n",
    "estimators = [ ('svm', svm), ('dt', dectr), ('nb', nb), ('knn', knn)]\n",
    "model = ensemble.VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "fitModel = model.fit(X,y)\n",
    "\n",
    "\n",
    "# https://datascienceplus.com/genetic-algorithm-in-machine-learning-using-python/\n",
    "\n",
    "def fitnessScore(population,yTest):\n",
    "    scores = []\n",
    "    for chromosome in population :\n",
    "        prediction = fitModel.predict(chromosome)\n",
    "        # We want the bad pred to be valued better\n",
    "        # 0 correspond to \"anomaly\" and 1 to \"normal\"\n",
    "        if(prediction == 0):\n",
    "            score = 1\n",
    "        else :\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    scores, population = np.array(scores), np.array(population) \n",
    "    inds = np.argsort(scores)\n",
    "    return list(scores[inds][::-1]), list(population[inds,:][::-1])\n",
    "\n",
    "''' # TO ADAPT\n",
    "def selection(pop_after_fit,n_parents):\n",
    "    population_nextgen = []\n",
    "    for i in range(n_parents):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "\n",
    "def crossover(pop_after_sel):\n",
    "    population_nextgen=pop_after_sel\n",
    "    for i in range(len(pop_after_sel)):\n",
    "        child=pop_after_sel[i]\n",
    "        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n",
    "        population_nextgen.append(child)\n",
    "    return population_nextgen\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate):\n",
    "    population_nextgen = []\n",
    "    for i in range(0,len(pop_after_cross)):\n",
    "        chromosome = pop_after_cross[i]\n",
    "        for j in range(len(chromosome)):\n",
    "            if random.random() < mutation_rate:\n",
    "                chromosome[j]= not chromosome[j]\n",
    "        population_nextgen.append(chromosome)\n",
    "    #print(population_nextgen)\n",
    "    return population_nextgen\n",
    "\n",
    "def generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n",
    "                                   X_test, y_train, y_test):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(size,n_feat)\n",
    "    for i in range(n_gen):\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "        print(scores[:2])\n",
    "        pop_after_sel = selection(pop_after_fit,n_parents)\n",
    "        pop_after_cross = crossover(pop_after_sel)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "    return best_chromo,best_score\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but est de maximiser le nombre de vecteurs malsain classés en sain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarn Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du *Particle Swarm Optimization* est que pour chaque particule d'une nuée, on va essayer de trouver la meilleure position retournée par la *fitness function*. \n",
    "\n",
    "- D'abord, on doit créer la nuée de 200 particules. Les particules sont formées à partir d'une rangée du dataset et d'une partie généré aléatoirement, toujours avec les mêmes *features* que le dataset. Comme que pour le GA, on devrait prendre seulement la partie mutable des particules à modifier. Ensuites, pour chaque particules est attribué une vélocité de 0.7.\n",
    "- Pour chaque itération, on va évaluer chaque particule à partir de la *fitness function*. On met à jour la meilleur valeur de la fonction fitness possible de chaque particules (de 0.5 au départ) et de la meilleur valeur de la fonction fitness global (de 0.4 au départ)\n",
    "- On calcule en suite la distance entre la position de chaque particule et on récupère la meilleure valeur grâce à la fonction de fitness. On met à jour la vélocité et la position de chaque particule avec les valeurs précédentes.\n",
    "- On répète tout ce processus 100 fois au maximum ou s'il y a une amélioration de moins de 0.001%.\n",
    "La nué de particules devient alors un nouveau dataset de vecteurs *malsains*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyswarms in /home/jp/.local/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: future in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (0.18.2)\n",
      "Requirement already satisfied: tqdm in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (4.53.0)\n",
      "Requirement already satisfied: pyyaml in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (5.3.1)\n",
      "Requirement already satisfied: scipy in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (3.3.2)\n",
      "Requirement already satisfied: attrs in /usr/lib/python3.8/site-packages (from pyswarms) (20.3.0)\n",
      "Requirement already satisfied: numpy in /home/jp/.local/lib/python3.8/site-packages (from pyswarms) (1.19.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/jp/.local/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/jp/.local/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/jp/.local/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (7.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jp/.local/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=1.3.1->pyswarms) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarms.single import GlobalBestPSO\n",
    "from pyswarms.utils.search.grid_search  import SearchBase\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def prepare_UNSW_NB15(data):\n",
    "    LE = LabelEncoder()\n",
    "    copy = data[data['label']==1]\n",
    "    copy = copy.drop(['label'], axis=1)\n",
    "    copy['proto'] = LE.fit_transform(copy['proto'])\n",
    "    copy['service'] = LE.fit_transform(copy['service'])\n",
    "    copy['state'] = LE.fit_transform(copy['state'])\n",
    "    copy['attack_cat'] = LE.fit_transform(copy['attack_cat'])\n",
    "    return copy\n",
    "\n",
    "def voting_classifier(X_train, Y_train) :\n",
    "  sv = svm.SVC(gamma = 0.01, C = 220.0, tol = 0.01, probability = True)\n",
    "  dt = DecisionTreeClassifier(\n",
    "      criterion = \"entropy\", min_samples_split = 4, min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "  nb = GaussianNB()\n",
    "  kn = KNeighborsClassifier( n_neighbors = 3, algorithm = \"auto\")\n",
    "\n",
    "  vC = VotingClassifier (estimators=\n",
    "                         [('svm', sv), ('dt', dt), ('nb', nb), ('knn',kn)], voting='soft')\n",
    "  vC = vC.fit(X_train,Y_train)\n",
    "  return vC\n",
    "\n",
    "def Pso (dim, data) :\n",
    "  option = {'c1': 0.5, 'c2': 0.4, 'w' : 0.7}\n",
    "  opt = GlobalBestPSO (n_particles=200, dimensions=dim, options=option)\n",
    "  return opt.optimize(prepare_UNSW_NB15, 100, None, True, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#df2prepare = Pso (len(copy.columns), df2)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df2prepare, test_size=0.2, random_state=300)\n",
    "\n",
    "#voting_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Generative Adversial Network* est une technique de deep learning qui va confronter deux réseaux de neuronnes. Ces deux réseaux de neuronnes correspondent l'un à la génération des vecteur *malicieux* et donc à ceux qu'elles permettent de tromper, l'autre est le discriminateur, qui lui va essayer de deviner si l'entrée est saine ou non.\n",
    "Comme les méthodes précédentes, on va modifier d'abord la partie modifiable, c'est à dire non fixée des vecteurs. \n",
    "- Pour le générateur, on va sélectionner aléatoirement un vecteur \"malsain\". On va ajouter du bruit dans certaines des features choisies au hasard, toujours dans la partie modifiable du vecteur malicieux. Puis ce vecteur va rentrer dans le réseau neuronal du générateur avec comme sortie, un vecteur de la taille de la partie mutable du vecteur d'entrée. On recombine le vecteur de sortie avec la partie non changeable du vecteur de l'entrée, et on le donne en entrée du discriminateur. Les labels sortant de ce dernier permettrons d'améliorer le réseau de neurone du générateur.\n",
    "- Pour le discriminateur, on va faire la même chose avec mais un vecteur bénin.\n",
    "- Les deux réseaux de neurones utiliserons des sets de données d'entraînements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les expériences sont enfin réalisée sur les 11 modèles de classification en utilisant les sets produits précédemment. \n",
    "L'article ne mentionne pas les hyperparamètres à appliquer pour les 11 classifieurs. \n",
    "Les 11 classifieurs sont Support Vector Machine (SVM), Decision Trees (DT), Naive Bayes (NB), k Nearest Neighbors (KNN), Random Forest (RF), Multi-layer Perceptron (MLP), Gradient Boosting (GB), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Bagging (BAG)\n",
    "Nous laisserons principalement les paramètres par défaut pour réaliser les expériences.\n",
    "Les résultats attendus pour ces expériences sont un taux élevé de mauvaise classification (dans le cas où nous arriverions à générer les adversairal datasets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(clf, X_train, Y_train, X_test, Y_test):\n",
    "    model = clf();\n",
    "    model.fit(X_train, Y_train);\n",
    "    return model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparaison (clf, X_train, Y_train, X_test, Y_test) :\n",
    "    score = []\n",
    "    for i in clf :\n",
    "        score.append(classifier(i, X_train, Y_train, X_test, Y_test))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_UNSW_NB15(df):\n",
    "    df2copy = df.copy()\n",
    "    LE = LabelEncoder()\n",
    "    label = df.loc[:,('label')]\n",
    "    df2copy = df2copy.drop(['label'], axis=1)\n",
    "    df2copy['proto'] = LE.fit_transform(df2copy['proto'])\n",
    "    df2copy['service'] = LE.fit_transform(df2copy['service'])\n",
    "    df2copy['state'] = LE.fit_transform(df2copy['state'])\n",
    "    df2copy['attack_cat'] = LE.fit_transform(df2copy['attack_cat'])\n",
    "    return df2copy, label\n",
    "\n",
    "df2copy, label = prepareDataset(df2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2copy, label, test_size=0.33, random_state=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X_train,y_train, X_test, y_test):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train,y_train)\n",
    "    return(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=500)\n",
    "#comparaison(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baggingexp(X_train,y_train, X_test, y_test):\n",
    "    \n",
    "    #df2copy = prepareDataset(dataset)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df2copy, label, test_size=0.5, random_state=500)\n",
    "    \n",
    "\n",
    "    model = BaggingClassifier(base_estimator=None, n_estimators=10, random_state=0).fit(X_train, y_train)\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "#     n_scores = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#     return model.score(X_test, y_test)\n",
    "    return model.score(X_test, y_test)\n",
    "#     print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVMc(df):\n",
    "    df = prepareKDD(df)\n",
    "    y = df['class']\n",
    "    df.drop(['class'], axis='columns', inplace=True)\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train,y_train)\n",
    "    return(svm.score(X_test, y_test))\n",
    "\n",
    "def prepareKDD(df):\n",
    "    trainCopy = df.copy()\n",
    "    # limit cause we can't run it on 22000 entries.\n",
    "    trainCopy = trainCopy.head(3000)\n",
    "\n",
    "    # preparation of the data : transforming string input to int \n",
    "    LE = LabelEncoder()\n",
    "    trainCopy['protocol_type'] = LE.fit_transform(trainCopy['protocol_type'])\n",
    "    trainCopy['service'] = LE.fit_transform(trainCopy['service'])\n",
    "    trainCopy['flag'] = LE.fit_transform(trainCopy['flag'])\n",
    "    trainCopy['class'] = LE.fit_transform(trainCopy['class'])\n",
    "    return trainCopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appels de tous les classifieurs et récupération des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.8492300779427268, 0.8988991237924062, 0.6253333333333333, 1.0]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = [DecisionTreeClassifier, GaussianNB, KNeighborsClassifier]\n",
    "\n",
    "score = comparaison(clf, X_train, y_train, X_test, y_test)\n",
    "score.append(SVMc(df))\n",
    "score.append(baggingexp(X_train,y_train, X_test, y_test))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAASL0lEQVR4nO3df7DddZ3f8eeL3JCkAu7AvZ1iAty0BjRjgGiIv2qbQsCgndBO7QY6rAXZTasLI7vOtmhX22bHGawz27Vudk2qmO0WBSqdNYNpYa3Zsa5gEwIiAYNZloVLM0tIRcE1GyLv/nFP4Bpuck/gJCf3c56PmTtzv9/z4Zz3OWOe8/V7zvneVBWSpOnvhH4PIEnqDYMuSY0w6JLUCIMuSY0w6JLUiKF+PfDw8HCNjo726+ElaVq69957n66qkclu61vQR0dH2bp1a78eXpKmpSR/cajbPOUiSY0w6JLUCIMuSY3o2zl06XCef/55xsbG2Lt3b79HOaZmz57NvHnzmDlzZr9H0TRk0HVcGhsb4+STT2Z0dJQk/R7nmKgq9uzZw9jYGPPnz+/3OJqGPOWi49LevXs57bTTBibmAEk47bTTBu7/lah3pgx6kpuSPJXkwUPcniT/KcnOJA8keXPvx9QgGqSYHzCIz1m9080R+gZgxWFuvxRY0PlZDfz+qx9LknSkpjyHXlXfTDJ6mCWXAf+lxi+sfk+SX0hyelXt6tWQ0ugNX+vp/T1243unXPPJT36SL33pS8yYMYMTTjiBdevW8da3vrWnc0i91Is3RecCT0zYHuvse1nQk6xm/CieM88881U/cK//kb8S3YRB08/dd9/NHXfcwbZt25g1axZPP/00+/bte8X3t3//foaG/AxCPw1CL47pm6JVtb6qllTVkpGRSS9FIB0Xdu3axfDwMLNmzQJgeHiY173udWzZsoV3vOMdnHfeeSxdupRnn32WvXv3cvXVV7No0SIWL17M5s2bAdiwYQMrV67kwgsv5KKLLuInP/kJH/jAB1i6dCmLFy/mq1/9aj+fohrUi0OGJ4EzJmzP6+yTpq1LLrmENWvWcPbZZ7N8+XJWrVrF29/+dlatWsWtt97KBRdcwI9//GPmzJnDZz7zGZLwve99j+9///tccsklPPLIIwBs27aNBx54gFNPPZWPfexjXHjhhdx0000888wzLF26lOXLl/Oa17ymz89WrejFEfpG4P2dT7u8DfiR58813Z100knce++9rF+/npGREVatWsW6des4/fTTueCCCwA45ZRTGBoa4lvf+hZXXnklAG94wxs466yzXgz6xRdfzKmnngrAXXfdxY033sj555/PsmXL2Lt3L48//nh/nqCaNOURepIvA8uA4SRjwL8FZgJU1eeATcB7gJ3AXwFXH61hpWNpxowZLFu2jGXLlrFo0SLWrl17xPcx8ei7qrj99ts555xzejmm9KIpj9Cr6oqqOr2qZlbVvKr6QlV9rhNzatyvVtXfqapFVeU1cTXt7dixgx/84Acvbt9///288Y1vZNeuXWzZsgWAZ599lv379/Oud72Lm2++GYBHHnmExx9/fNJov/vd7+azn/0s4x8Ig/vuu+8YPBMNEt9217RwrD9N9Nxzz3HdddfxzDPPMDQ0xOtf/3rWr1/P1VdfzXXXXcdPf/pT5syZw9e//nU+9KEP8cEPfpBFixYxNDTEhg0bXnwzdaKPf/zjXH/99Zx77rm88MILzJ8/nzvuuOOYPi+1zaBLk3jLW97Ct7/97ZftHx4e5p577nnZ/i9+8Ysv23fVVVdx1VVXvbg9Z84c1q1b19M5pYm8loskNcKgS1IjDLqOWwfePBwkg/ic1TsGXcel2bNns2fPnoEK3IHroc+ePbvfo2ia8k1RHZfmzZvH2NgYu3fv7vcox9SBv1gkvRIGXcelmTNn+ld7pCPkKRdJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoSfQ1dzBuGPAUuT8Qhdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrhN0Ub4bcjJXmELkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1Iiugp5kRZIdSXYmuWGS289MsjnJfUkeSPKe3o8qSTqcKYOeZAawFrgUWAhckWThQct+E7itqhYDlwO/1+tBJUmH180R+lJgZ1U9WlX7gFuAyw5aU8Apnd9fC/zf3o0oSepGN0GfCzwxYXuss2+ifwdcmWQM2ARcN9kdJVmdZGuSrbt3734F40qSDqVXb4peAWyoqnnAe4A/TPKy+66q9VW1pKqWjIyM9OihJUnQXdCfBM6YsD2vs2+ia4DbAKrqbmA2MNyLASVJ3ekm6FuABUnmJzmR8Tc9Nx605nHgIoAkb2Q86J5TkaRjaMqgV9V+4FrgTuBhxj/Nsj3JmiQrO8s+AvxKku8CXwauqqo6WkNLkl6uq8vnVtUmxt/snLjvExN+fwh4Z29HkyQdCb8pKkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1Iihfg8g6egZveFr/R6Bx258b79HGBgeoUtSI7oKepIVSXYk2ZnkhkOs+cUkDyXZnuRLvR1TkjSVKU+5JJkBrAUuBsaALUk2VtVDE9YsAD4KvLOqfpjkbx6tgSVJk+vmCH0psLOqHq2qfcAtwGUHrfkVYG1V/RCgqp7q7ZiSpKl0E/S5wBMTtsc6+yY6Gzg7yZ8muSfJil4NKEnqTq8+5TIELACWAfOAbyZZVFXPTFyUZDWwGuDMM8/s0UNLkqC7I/QngTMmbM/r7JtoDNhYVc9X1Z8DjzAe+J9TVeuraklVLRkZGXmlM0uSJtFN0LcAC5LMT3IicDmw8aA1f8T40TlJhhk/BfNo78aUJE1lyqBX1X7gWuBO4GHgtqranmRNkpWdZXcCe5I8BGwGfqOq9hytoSVJL9fVOfSq2gRsOmjfJyb8XsCvd34kSX3gN0UlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRFdBT3JiiQ7kuxMcsNh1v2TJJVkSe9GlCR1Y8qgJ5kBrAUuBRYCVyRZOMm6k4EPA9/p9ZCSpKl1c4S+FNhZVY9W1T7gFuCySdb9FvApYG8P55MkdamboM8FnpiwPdbZ96IkbwbOqKqvHe6OkqxOsjXJ1t27dx/xsJKkQ3vVb4omOQH4beAjU62tqvVVtaSqloyMjLzah5YkTdBN0J8EzpiwPa+z74CTgTcBf5LkMeBtwEbfGJWkY6uboG8BFiSZn+RE4HJg44Ebq+pHVTVcVaNVNQrcA6ysqq1HZWJJ0qSmDHpV7QeuBe4EHgZuq6rtSdYkWXm0B5QkdWeom0VVtQnYdNC+Txxi7bJXP5Yk6Uj5TVFJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGGHRJaoRBl6RGdBX0JCuS7EiyM8kNk9z+60keSvJAkv+V5KzejypJOpwpg55kBrAWuBRYCFyRZOFBy+4DllTVucBXgP/Q60ElSYfXzRH6UmBnVT1aVfuAW4DLJi6oqs1V9VedzXuAeb0dU5I0lW6CPhd4YsL2WGffoVwD/I/JbkiyOsnWJFt3797d/ZSSpCn19E3RJFcCS4BPT3Z7Va2vqiVVtWRkZKSXDy1JA2+oizVPAmdM2J7X2fdzkiwH/g3w96vqr3szniSpW90coW8BFiSZn+RE4HJg48QFSRYD64CVVfVU78eUJE1lyqBX1X7gWuBO4GHgtqranmRNkpWdZZ8GTgL+W5L7k2w8xN1Jko6Sbk65UFWbgE0H7fvEhN+X93guSdIR8puiktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktQIgy5JjTDoktSIroKeZEWSHUl2JrlhkttnJbm1c/t3koz2fFJJ0mFNGfQkM4C1wKXAQuCKJAsPWnYN8MOqej3wH4FP9XpQSdLhdXOEvhTYWVWPVtU+4BbgsoPWXAb8Qef3rwAXJUnvxpQkTSVVdfgFyfuAFVX1y53tXwLeWlXXTljzYGfNWGf7zzprnj7ovlYDqzub5wA7evVEXoVh4OkpVw0GX4txvg4v8bV4yfHyWpxVVSOT3TB0LKeoqvXA+mP5mFNJsrWqlvR7juOBr8U4X4eX+Fq8ZDq8Ft2ccnkSOGPC9rzOvknXJBkCXgvs6cWAkqTudBP0LcCCJPOTnAhcDmw8aM1G4J93fn8f8I2a6lyOJKmnpjzlUlX7k1wL3AnMAG6qqu1J1gBbq2oj8AXgD5PsBP4f49GfLo6rU0B95msxztfhJb4WLznuX4sp3xSVJE0PflNUkhph0CWpEQMb9KkuZzAoktyU5KnOdwkGWpIzkmxO8lCS7Uk+3O+Z+iXJ7CT/J8l3O6/Fv+/3TP2WZEaS+5Lc0e9ZDmUgg97l5QwGxQZgRb+HOE7sBz5SVQuBtwG/OsD/u/hr4MKqOg84H1iR5G39HanvPgw83O8hDmcgg053lzMYCFX1TcY/mTTwqmpXVW3r/P4s4/945/Z3qv6occ91Nmd2fgb2ExRJ5gHvBT7f71kOZ1CDPhd4YsL2GAP6D1eT61wxdDHwnT6P0jedUwz3A08Bf1xVA/taAL8D/CvghT7PcViDGnTpkJKcBNwOXF9VP+73PP1SVT+rqvMZ/3b40iRv6vNIfZHkHwJPVdW9/Z5lKoMa9G4uZ6ABlGQm4zG/uar+e7/nOR5U1TPAZgb3vZZ3AiuTPMb46dkLk/zX/o40uUENejeXM9CA6Vzy+QvAw1X12/2ep5+SjCT5hc7vc4CLge/3dag+qaqPVtW8qhplvBXfqKor+zzWpAYy6FW1HzhwOYOHgduqant/p+qPJF8G7gbOSTKW5Jp+z9RH7wR+ifEjsPs7P+/p91B9cjqwOckDjB8A/XFVHbcf19M4v/ovSY0YyCN0SWqRQZekRhh0SWqEQZekRhh0SWqEQde0k+RvJbklyZ8luTfJpiRn9/KKkUnWJFne+f1dnSsO3p9kbpKv9OpxpF7yY4uaVjpf/vk28AdV9bnOvvOAU4Dfr6qefz09yeeAb1VVT78dmGRGVf2sl/epweYRuqabfwA8fyDmAFX1XSZcbC3JaJL/nWRb5+cdnf2nJ/lm50j7wc6R94wkGzrb30vya521G5K8L8kvA78I/FaSmzv3/WBnzYwkn06yJckDSf5FZ/+yidfMTvK7Sa7q/P5Ykk8l2Qb806P9YmmwTPlHoqXjzJuAqS6S9BRwcVXtTbIA+DKwBPhnwJ1V9cnONfH/BuPX+p574Mj+wNfdD6iqzyf5u8AdVfWVzlUYD7gG+FFVXZBkFvCnSe7q4jnsqao3d7FOOiIGXS2aCfxukvOBnwFnd/ZvAW7qXIDrj6rq/iSPAn87yWeBrwHdBPmAS4Bzk7yvs/1aYAGwb4r/7tYjeAypa55y0XSzHXjLFGt+DfhL4DzGj8xPhBf/mMffY/zKmhuSvL+qfthZ9yfAv+TI/oBBgOuq6vzOz/yquovxv3w08d/W7IP+u58cwWNIXTPomm6+AcxKsvrAjiTn8vOXQ34tsKuqXmD8YlszOuvOAv6yqv4z4+F+c5Jh4ISquh34TeBIToXcCXywc8RP55M2rwH+AliYZFbnFM5Fr+ypSkfGUy6aVqqqkvxj4HeS/GtgL/AYcP2EZb8H3J7k/cD/5KUj4mXAbyR5HngOeD/jf6nqi0kOHNx89AjG+TwwCmzrfPpmN/CPquqJJLcBDwJ/Dtx3hE9TekX82KIkNcJTLpLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUiP8P55a6As/KcSYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotClassifieurBarScore(score):\n",
    "    classifieur = []\n",
    "    for i in range (len(score)):\n",
    "        classifieur.insert(i,i)\n",
    "    plotDF = pd.DataFrame({'Classifieur':classifieur, 'Score':score})\n",
    "    plotDF.plot.bar(x='Classifieur', y='Score', rot=0)\n",
    "\n",
    "plotClassifieurBarScore(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Améliorer le plot en rajoutant la liste des classifieurs au lieu d'avoir une liste de nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
