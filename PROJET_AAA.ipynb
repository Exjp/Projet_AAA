{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Machine Learning in Network Intrusion Detection Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'article utilise comme dataset NSL-KDD et UNSW-NB15, qui représente des flux de données bruts. Ces flux sont un mélanges différent type de trafic mais aussi un mélange  de bonnes et malicieuses données. Le but étant d'altérer ces paquets de données pour pouvoir passer les modèles de machine learning de sécurité tout en gardant en gardant les flux de donnée fonctionnels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22544 entries, 0 to 22543\n",
      "Data columns (total 42 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   duration                     22544 non-null  int64  \n",
      " 1   protocol_type                22544 non-null  object \n",
      " 2   service                      22544 non-null  object \n",
      " 3   flag                         22544 non-null  object \n",
      " 4   src_bytes                    22544 non-null  int64  \n",
      " 5   dst_bytes                    22544 non-null  int64  \n",
      " 6   land                         22544 non-null  int64  \n",
      " 7   wrong_fragment               22544 non-null  int64  \n",
      " 8   urgent                       22544 non-null  int64  \n",
      " 9   hot                          22544 non-null  int64  \n",
      " 10  num_failed_logins            22544 non-null  int64  \n",
      " 11  logged_in                    22544 non-null  int64  \n",
      " 12  num_compromised              22544 non-null  int64  \n",
      " 13  root_shell                   22544 non-null  int64  \n",
      " 14  su_attempted                 22544 non-null  int64  \n",
      " 15  num_root                     22544 non-null  int64  \n",
      " 16  num_file_creations           22544 non-null  int64  \n",
      " 17  num_shells                   22544 non-null  int64  \n",
      " 18  num_access_files             22544 non-null  int64  \n",
      " 19  num_outbound_cmds            22544 non-null  int64  \n",
      " 20  is_host_login                22544 non-null  int64  \n",
      " 21  is_guest_login               22544 non-null  int64  \n",
      " 22  count                        22544 non-null  int64  \n",
      " 23  srv_count                    22544 non-null  int64  \n",
      " 24  serror_rate                  22544 non-null  float64\n",
      " 25  srv_serror_rate              22544 non-null  float64\n",
      " 26  rerror_rate                  22544 non-null  float64\n",
      " 27  srv_rerror_rate              22544 non-null  float64\n",
      " 28  same_srv_rate                22544 non-null  float64\n",
      " 29  diff_srv_rate                22544 non-null  float64\n",
      " 30  srv_diff_host_rate           22544 non-null  float64\n",
      " 31  dst_host_count               22544 non-null  int64  \n",
      " 32  dst_host_srv_count           22544 non-null  int64  \n",
      " 33  dst_host_same_srv_rate       22544 non-null  float64\n",
      " 34  dst_host_diff_srv_rate       22544 non-null  float64\n",
      " 35  dst_host_same_src_port_rate  22544 non-null  float64\n",
      " 36  dst_host_srv_diff_host_rate  22544 non-null  float64\n",
      " 37  dst_host_serror_rate         22544 non-null  float64\n",
      " 38  dst_host_srv_serror_rate     22544 non-null  float64\n",
      " 39  dst_host_rerror_rate         22544 non-null  float64\n",
      " 40  dst_host_srv_rerror_rate     22544 non-null  float64\n",
      " 41  class                        22544 non-null  object \n",
      "dtypes: float64(15), int64(23), object(4)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"KDDTest+.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175341 entries, 0 to 175340\n",
      "Data columns (total 45 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   id                 175341 non-null  int64  \n",
      " 1   dur                175341 non-null  float64\n",
      " 2   proto              175341 non-null  object \n",
      " 3   service            175341 non-null  object \n",
      " 4   state              175341 non-null  object \n",
      " 5   spkts              175341 non-null  int64  \n",
      " 6   dpkts              175341 non-null  int64  \n",
      " 7   sbytes             175341 non-null  int64  \n",
      " 8   dbytes             175341 non-null  int64  \n",
      " 9   rate               175341 non-null  float64\n",
      " 10  sttl               175341 non-null  int64  \n",
      " 11  dttl               175341 non-null  int64  \n",
      " 12  sload              175341 non-null  float64\n",
      " 13  dload              175341 non-null  float64\n",
      " 14  sloss              175341 non-null  int64  \n",
      " 15  dloss              175341 non-null  int64  \n",
      " 16  sinpkt             175341 non-null  float64\n",
      " 17  dinpkt             175341 non-null  float64\n",
      " 18  sjit               175341 non-null  float64\n",
      " 19  djit               175341 non-null  float64\n",
      " 20  swin               175341 non-null  int64  \n",
      " 21  stcpb              175341 non-null  int64  \n",
      " 22  dtcpb              175341 non-null  int64  \n",
      " 23  dwin               175341 non-null  int64  \n",
      " 24  tcprtt             175341 non-null  float64\n",
      " 25  synack             175341 non-null  float64\n",
      " 26  ackdat             175341 non-null  float64\n",
      " 27  smean              175341 non-null  int64  \n",
      " 28  dmean              175341 non-null  int64  \n",
      " 29  trans_depth        175341 non-null  int64  \n",
      " 30  response_body_len  175341 non-null  int64  \n",
      " 31  ct_srv_src         175341 non-null  int64  \n",
      " 32  ct_state_ttl       175341 non-null  int64  \n",
      " 33  ct_dst_ltm         175341 non-null  int64  \n",
      " 34  ct_src_dport_ltm   175341 non-null  int64  \n",
      " 35  ct_dst_sport_ltm   175341 non-null  int64  \n",
      " 36  ct_dst_src_ltm     175341 non-null  int64  \n",
      " 37  is_ftp_login       175341 non-null  int64  \n",
      " 38  ct_ftp_cmd         175341 non-null  int64  \n",
      " 39  ct_flw_http_mthd   175341 non-null  int64  \n",
      " 40  ct_src_ltm         175341 non-null  int64  \n",
      " 41  ct_srv_dst         175341 non-null  int64  \n",
      " 42  is_sm_ips_ports    175341 non-null  int64  \n",
      " 43  attack_cat         175341 non-null  object \n",
      " 44  label              175341 non-null  int64  \n",
      "dtypes: float64(11), int64(30), object(4)\n",
      "memory usage: 60.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"UNSW_NB15_training-set.csv\")\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Genetic algorithm* (GA) est un algorithme qui se base sur la génétique et la sélection naturelle. Le GA utilise une forme de données appelées chromosome : ces données (vecteurs) sont formées et les valeurs de différentes features sont destinées à évoluer comme pour un gêne d'être vivant.\n",
    "Certaines *features* sont inchangeable du à leur importance dans le bon fonctionnement des paquets.\n",
    "\n",
    "- En premier lieu, on va créer une population de 100 chromosomes, dont leurs données seront générer aléatoirement, séparées en 2 parties, les données que l'ont peu modifier et les autres. (pour les opérations qui suivent, on va utiliser la partie que l'on peut modifier)\n",
    "- on va ensuite utiliser l'opération de *cross over*. On choisi aléatoirement 2 chromosomes que l'on va séparer l'un de 25% de ses features et donc, de l'autre des 75% autres, que l'on va échanger pour créer 2 nouveau chromosomes. Ces nouveaux chromosomes seront ensuite insérés dans la population.\n",
    "- Enfin, on va utiliser l'opération de *mutation* sur la partie que l'on peut modifier, qui consiste à choisir de manière aléatoire un chromosome et de même pour une de ses features. Toute cette opération est exécutée \n",
    "- On regroupe les 2 parties des chromosomes.\n",
    "- Finalement, on va utiliser une *fitness function* pour évaluer chaque chromosome, s'ils sont bénins (seuil à 99,99%) ou non. On retiendra le meilleur chromosome toutes les 5 générations.\n",
    "- On répète tout ce processus 100 fois\n",
    "On obtient alors un nouveau dataset ne contenant que des vecteurs \"malsains\" qui seront testés dans la partie expérience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from geneticalgorithm import geneticalgorithm as ga\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Creation of initial population\n",
    "listProtocols = [\"tcp\",\"udp\",\"icmp\"]\n",
    "listData = [\"private\",\"ftp_data\",\"eco_i\",\"telnet\",\"http\",\"smtp\",\"imap4\",\"systat\",\"pop3\",\"domain_u\",\"whois\",\"netbios_dgm\"]\n",
    "listSF = [\"SF\",\"S0\",\"REJ\",\"RSTR\"]\n",
    "#cell 3 = 62825648 - 0\n",
    "# 24 à 30 et 33 à 40 entre 0 et 1\n",
    "# 4 à 23 , 31- 32,  entre 0 et 20\n",
    "#derniere = 0 - 21\n",
    "\n",
    "# creation of artifical dataset\n",
    "def initialPopulation(df, nbChrom):\n",
    "    initPopulation = []\n",
    "    for i in range (nbChrom):\n",
    "        chrom = []\n",
    "        for j in range (len(df.columns)):\n",
    "            if(j == 0) :\n",
    "                chrom.insert(j,random.randint(0, 2))\n",
    "            if(j == 1) :\n",
    "                chrom.insert(j,random.randint(0, 11))\n",
    "            if(j == 2) :\n",
    "                chrom.insert(j,randint(0, 62825648))\n",
    "            if((j >= 24 and j <= 30) or (j >= 33 and j <= 40)) :\n",
    "                chrom.insert(j,random.uniform(0, 1))\n",
    "            if((j >= 4 and j <= 23) or j == 32 or j ==33) :\n",
    "                chrom.insert(j,random.randint(0, 62825648))\n",
    "            if(j == 41) :\n",
    "                chrom.insert(j,random.randint(0,21))\n",
    "        initPopulation.append(chrom)\n",
    "    return initPopulation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimDF = len(df.columns)\n",
    "dimDF2 = len(df2.columns)\n",
    "\n",
    "#We have to reduce to a minimum data size otherwise our computers can not run the fit after...\n",
    "dfCopy = df.copy()\n",
    "dfCopy = dfCopy.head(1000)\n",
    "\n",
    "# preparation of the data : transforming string input to int \n",
    "LE = LabelEncoder()\n",
    "dfCopy['protocol_type'] = LE.fit_transform(dfCopy['protocol_type'])\n",
    "dfCopy['service'] = LE.fit_transform(dfCopy['service'])\n",
    "dfCopy['flag'] = LE.fit_transform(dfCopy['flag'])\n",
    "dfCopy['class'] = LE.fit_transform(dfCopy['class'])\n",
    "dfCopy.head()\n",
    "\n",
    "y = dfCopy['class']\n",
    "dfCopy.drop(['class'], axis='columns', inplace=True)\n",
    "X = dfCopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "population = initialPopulation(dfCopy,100)\n",
    "#fitnessScore(population,)\n",
    "#population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # TO ADAPT\\ndef selection(pop_after_fit,n_parents):\\n    population_nextgen = []\\n    for i in range(n_parents):\\n        population_nextgen.append(pop_after_fit[i])\\n    return population_nextgen\\n\\ndef crossover(pop_after_sel):\\n    population_nextgen=pop_after_sel\\n    for i in range(len(pop_after_sel)):\\n        child=pop_after_sel[i]\\n        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\\n        population_nextgen.append(child)\\n    return population_nextgen\\n\\ndef mutation(pop_after_cross,mutation_rate):\\n    population_nextgen = []\\n    for i in range(0,len(pop_after_cross)):\\n        chromosome = pop_after_cross[i]\\n        for j in range(len(chromosome)):\\n            if random.random() < mutation_rate:\\n                chromosome[j]= not chromosome[j]\\n        population_nextgen.append(chromosome)\\n    #print(population_nextgen)\\n    return population_nextgen\\n\\ndef generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\\n                                   X_test, y_train, y_test):\\n    best_chromo= []\\n    best_score= []\\n    population_nextgen=initilization_of_population(size,n_feat)\\n    for i in range(n_gen):\\n        scores, pop_after_fit = fitness_score(population_nextgen)\\n        print(scores[:2])\\n        pop_after_sel = selection(pop_after_fit,n_parents)\\n        pop_after_cross = crossover(pop_after_sel)\\n        population_nextgen = mutation(pop_after_cross,mutation_rate)\\n        best_chromo.append(pop_after_fit[0])\\n        best_score.append(scores[0])\\n    return best_chromo,best_score\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Parameters given in the section 4.3 of the article\n",
    "svm = svm.SVC(C = 220, gamma = 0.01, probability = True, tol = 0.001)\n",
    "#  decision trees\n",
    "dectr = tree.DecisionTreeClassifier(criterion = 'entropy', min_samples_split = 4, \\\n",
    "                                    min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "#naive bayes\n",
    "nb = GaussianNB()\n",
    "#k nearest neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors = 3, algorithm ='auto')\n",
    "\n",
    "estimators = [ ('svm', svm), ('dt', dectr), ('nb', nb), ('knn', knn)]\n",
    "model = ensemble.VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "fitModel = model.fit(X,y)\n",
    "\n",
    "\n",
    "# https://datascienceplus.com/genetic-algorithm-in-machine-learning-using-python/\n",
    "\n",
    "def fitnessScore(population,yTest):\n",
    "    scores = []\n",
    "    for chromosome in population :\n",
    "        prediction = fitModel.predict(chromosome)\n",
    "        # We want the bad pred to be valued better\n",
    "        # 0 correspond to \"anomaly\" and 1 to \"normal\"\n",
    "        if(prediction == 0):\n",
    "            score = 1\n",
    "        else :\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    scores, population = np.array(scores), np.array(population) \n",
    "    inds = np.argsort(scores)\n",
    "    return list(scores[inds][::-1]), list(population[inds,:][::-1])\n",
    "\n",
    "''' # TO ADAPT\n",
    "def selection(pop_after_fit,n_parents):\n",
    "    population_nextgen = []\n",
    "    for i in range(n_parents):\n",
    "        population_nextgen.append(pop_after_fit[i])\n",
    "    return population_nextgen\n",
    "\n",
    "def crossover(pop_after_sel):\n",
    "    population_nextgen=pop_after_sel\n",
    "    for i in range(len(pop_after_sel)):\n",
    "        child=pop_after_sel[i]\n",
    "        child[3:7]=pop_after_sel[(i+1)%len(pop_after_sel)][3:7]\n",
    "        population_nextgen.append(child)\n",
    "    return population_nextgen\n",
    "\n",
    "def mutation(pop_after_cross,mutation_rate):\n",
    "    population_nextgen = []\n",
    "    for i in range(0,len(pop_after_cross)):\n",
    "        chromosome = pop_after_cross[i]\n",
    "        for j in range(len(chromosome)):\n",
    "            if random.random() < mutation_rate:\n",
    "                chromosome[j]= not chromosome[j]\n",
    "        population_nextgen.append(chromosome)\n",
    "    #print(population_nextgen)\n",
    "    return population_nextgen\n",
    "\n",
    "def generations(size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n",
    "                                   X_test, y_train, y_test):\n",
    "    best_chromo= []\n",
    "    best_score= []\n",
    "    population_nextgen=initilization_of_population(size,n_feat)\n",
    "    for i in range(n_gen):\n",
    "        scores, pop_after_fit = fitness_score(population_nextgen)\n",
    "        print(scores[:2])\n",
    "        pop_after_sel = selection(pop_after_fit,n_parents)\n",
    "        pop_after_cross = crossover(pop_after_sel)\n",
    "        population_nextgen = mutation(pop_after_cross,mutation_rate)\n",
    "        best_chromo.append(pop_after_fit[0])\n",
    "        best_score.append(scores[0])\n",
    "    return best_chromo,best_score\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but est de maximiser le nombre de vecteurs malsain classés en sain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle Swarn Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée du *Particle Swarm Optimization* est que pour chaque particule d'une nuée, on va essayer de trouver la meilleure position retournée par la *fitness function*. \n",
    "\n",
    "- D'abord, on doit créer la nuée de 200 particules. Les particules sont formées à partir d'une rangée du dataset et d'une partie généré aléatoirement, toujours avec les mêmes *features* que le dataset. Comme que pour le GA, on devrait prendre seulement la partie mutable des particules à modifier. Ensuites, pour chaque particules est attribué une vélocité de 0.7.\n",
    "- Pour chaque itération, on va évaluer chaque particule à partir de la *fitness function*. On met à jour la meilleur valeur de la fonction fitness possible de chaque particules (de 0.5 au départ) et de la meilleur valeur de la fonction fitness global (de 0.4 au départ)\n",
    "- On calcule en suite la distance entre la position de chaque particule et on récupère la meilleure valeur grâce à la fonction de fitness. On met à jour la vélocité et la position de chaque particule avec les valeurs précédentes.\n",
    "- On répète tout ce processus 100 fois au maximum ou s'il y a une amélioration de moins de 0.001%.\n",
    "La nué de particules devient alors un nouveau dataset de vecteurs *malsains*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyswarms in /srv/conda/envs/notebook/lib/python3.6/site-packages (1.2.0)\n",
      "Requirement already satisfied: attrs in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (19.3.0)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (1.19.4)\n",
      "Requirement already satisfied: pyyaml in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (5.3.1)\n",
      "Requirement already satisfied: future in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (0.18.2)\n",
      "Requirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (4.53.0)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from pyswarms) (3.3.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from matplotlib>=1.3.1->pyswarms) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from matplotlib>=1.3.1->pyswarms) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib>=1.3.1->pyswarms) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarms.single import GlobalBestPSO\n",
    "from pyswarms.utils.search.grid_search  import SearchBase\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def prepare_UNSW_NB15(data):\n",
    "    LE = LabelEncoder()\n",
    "    copy = data[data['label']==1]\n",
    "    copy = copy.drop(['label'], axis=1)\n",
    "    copy['proto'] = LE.fit_transform(copy['proto'])\n",
    "    copy['service'] = LE.fit_transform(copy['service'])\n",
    "    copy['state'] = LE.fit_transform(copy['state'])\n",
    "    copy['attack_cat'] = LE.fit_transform(copy['attack_cat'])\n",
    "    return copy\n",
    "\n",
    "def voting_classifier(X_train, Y_train) :\n",
    "  sv = svm.SVC(gamma = 0.01, C = 220.0, tol = 0.01, probability = True)\n",
    "  dt = DecisionTreeClassifier(\n",
    "      criterion = \"entropy\", min_samples_split = 4, min_samples_leaf = 2, max_depth = 20, min_impurity_decrease = 0.1)\n",
    "  nb = GaussianNB()\n",
    "  kn = KNeighborsClassifier( n_neighbors = 3, algorithm = \"auto\")\n",
    "\n",
    "  vC = VotingClassifier (estimators=\n",
    "                         [('svm', sv), ('dt', dt), ('nb', nb), ('knn',kn)], voting='soft')\n",
    "  vC = vC.fit(X_train,Y_train)\n",
    "  return vC\n",
    "\n",
    "def Pso (dim, data) :\n",
    "  option = {'c1': 0.5, 'c2': 0.4, 'w' : 0.7}\n",
    "  opt = GlobalBestPSO (n_particles=200, dimensions=dim, options=option)\n",
    "  return opt.optimize(prepare_UNSW_NB15, 100, None, True, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-25 10:30:06,933 - pyswarms.single.global_best - INFO - Optimize for 100 iters with {'c1': 0.5, 'c2': 0.4, 'w': 0.7}\n",
      "pyswarms.single.global_best:   0%|          |0/100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "voting_classifier() got multiple values for argument 'X_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-6a4612ecb7d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mPso\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-9a335ee14122>\u001b[0m in \u001b[0;36mPso\u001b[0;34m(dim, x, y)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0moption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'c1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalBestPSO\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoting_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/pyswarms/single/global_best.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, objective_func, iters, n_processes, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# Compute cost for current position and personal best\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# fmt: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_objective_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbest_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbest_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_pbest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;31m# Set best_cost_yet_found for ftol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/pyswarms/backend/operators.py\u001b[0m in \u001b[0;36mcompute_objective_function\u001b[0;34m(swarm, objective_func, pool, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobjective_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswarm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         results = pool.map(\n",
      "\u001b[0;31mTypeError\u001b[0m: voting_classifier() got multiple values for argument 'X_train'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df2prepare = Pso (len(copy.columns), df2)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df2prepare, test_size=0.2, random_state=300)\n",
    "\n",
    "#voting_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *Generative Adversial Network* est une technique de deep learning qui va confronter deux réseaux de neuronnes. Ces deux réseaux de neuronnes correspondent l'un à la génération des vecteur *malicieux* et donc à ceux qu'elles permettent de tromper, l'autre est le discriminateur, qui lui va essayer de deviner si l'entrée est saine ou non.\n",
    "Comme les méthodes précédentes, on va modifier d'abord la partie modifiable, c'est à dire non fixée des vecteurs. \n",
    "- Pour le générateur, on va sélectionner aléatoirement un vecteur \"malsain\". On va ajouter du bruit dans certaines des features choisies au hasard, toujours dans la partie modifiable du vecteur malicieux. Puis ce vecteur va rentrer dans le réseau neuronal du générateur avec comme sortie, un vecteur de la taille de la partie mutable du vecteur d'entrée. On recombine le vecteur de sortie avec la partie non changeable du vecteur de l'entrée, et on le donne en entrée du discriminateur. Les labels sortant de ce dernier permettrons d'améliorer le réseau de neurone du générateur.\n",
    "- Pour le discriminateur, on va faire la même chose avec mais un vecteur bénin.\n",
    "- Les deux réseaux de neurones utiliserons des sets de données d'entraînements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les expériences sont enfin réalisée sur les 11 modèles de classification en utilisant les sets produits précédemment. \n",
    "L'article ne mentionne pas les hyperparamètres à appliquer pour les 11 classifieurs. \n",
    "Les 11 classifieurs sont Support Vector Machine (SVM), Decision Trees (DT), Naive Bayes (NB), k Nearest Neighbors (KNN), Random Forest (RF), Multi-layer Perceptron (MLP), Gradient Boosting (GB), Logistic Regression (LR), Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Bagging (BAG)\n",
    "Nous laisserons principalement les paramètres par défaut pour réaliser les expériences.\n",
    "Les résultats attendus pour ces expériences sont un taux élevé de mauvaise classification (dans le cas où nous arriverions à générer les adversairal datasets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
